{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DIVE Documentation","text":"<p>This is the documentation site for DIVE-DSA (DIVE - Digital Slide Archive), which is a fork based on DIVE a free and open-source annotation and analysis platform for web and desktop built by Kitware. This fork eliminates DIVE's integration with VIAME and provides some enhanced annotation features when compared to base DIVE.  It is also meant to be integrated in with Digital Slide Archive.</p> <p></p>"},{"location":"#dive-dsa-plugin","title":"DIVE-DSA Plugin","text":"<p>DIVE-DSA can be installed as a plugin into the Digital Slide Archive.  When installed as a plugin it provides the capbilities to annotate and view videos/image sequences in the DIVE interface.</p> <p>To access the home of DIVE when installed on a server with Digital Slide Archive the endpoint <code>localhost:8080/dive</code> will bring you to the core DIVE interface.</p>"},{"location":"#convert-videos-to-dive-format","title":"Convert Videos to DIVE Format","text":"<p>When videos are annotated in DIVE they need to be first processed and converted into a Dataset.  A Dataset is a folder which includes the source video and a transcoded video if required.</p> <p>When The DIVE-DSA plugin is installed and there is a Girder Item which contains a standard video extension (.mp4, .wmv, .move, .avi, ...others) there will be a button which will allow you to 'Convert to DIVE'. This button will create a folder with the name <code>Video {filename}</code> and process the video.  It will also launch the DIVE interface and navigate to the parent folder.  Once the job is complete you can click on 'Launch Annotator' to view the video and annotate.  For more information please see Video-Conversion</p>"},{"location":"#open-existing-dive-datasets","title":"Open Existing DIVE Datasets","text":"<p>Folders that have already been converted to DIVE can be open directly by clicking the 'Open in DIVE' button next to the folder name.  There is also an additional Open in DIVE button when viewing the contents of a folder that has been converted.</p>"},{"location":"#concepts-and-definitions","title":"Concepts and Definitions","text":"<p>DIVE is the annotator and data management software system.  It is our name for the code and capabilities, including both web and desktop, that can be deployed and configured for a variety of needs.  For this frok of DIVE (DIVE-DSA)</p> <p>Detection - A single annotation.  A detection could be associated with a point in time within a track, or it could have no temporal association.</p> <p>Features - Bounding box, polygon, head/tail points or other visible elements of a detection.</p> <p>Track - A collection of detections spanned over multiple frames in a video or image sequence.  Tracks include a start and end time and can have gap periods in which no detections exist.</p> <p>Group - A collection of one or more tracks, which can be given a definite frame range, type annotation, confidence, and attributes.</p> <p>Types - Every track (or detection, if tracks aren't applicable) has one or more types that should be used to annotate the primary characteristic you are interested in classifying.  Types are typically used to train a single or multi-class classifier.  A track (or detection) may have multiple types with confidence values associated.</p> <p>Frame - A single image or point in time for a video or image sequence.</p> <p>Key Frame - Every manually drawn annotation is considered a keyframe, and all automated pipelines produce keyframes. Only keyframes can have attributes.  Key frame detections are differentiated from interpolated detections, which are the implicit bounding boxes you see when linear interpolation is enabled.</p> <p>Interpolation - The implicit bounding boxes between keyframes in a track.</p> <p>Attributes - Attributes are free-form secondary characteristics on both tracks and detections. For example, a <code>fish</code> type track may have an <code>is_adult</code> boolean attribute.</p>"},{"location":"Annotation-QuickStart/","title":"Annotation Quickstart","text":"<p>Before following the quickstart, it could be helpful to skim the User Interface Guide</p>"},{"location":"Annotation-QuickStart/#single-frame-detections","title":"Single Frame Detections","text":"<p>How to quickly create multiple detections on a single image frame.</p> <ol> <li>Click  (creation settings menu) in the Track List area.<ol> <li>From the Mode dropdown, choose Detection.</li> <li>From the Type dropdown, choose or enter a default name that all new detections will have.  If the type doesn't exist yet, enter a name to create a new one.</li> <li>Turn on the Continuous Mode switch if you would like to automatically re-enter the creation state so you can click-and-drag repeatedly to quickly create many detections.</li> </ol> </li> <li>Enter the annotation creation state by clicking  Detection or pressing the N key.</li> <li>Create your first detection by clicking and dragging to draw a rectangle.</li> <li>If you are in continuous mode, click and drag again to create the next detection.<ol> <li>Press Esc to exit continuous creation mode.</li> </ol> </li> </ol>"},{"location":"Annotation-QuickStart/#single-detection-mode-demo","title":"Single Detection Mode Demo","text":"<p>The demo below shows how to use Detection mode to quickly create numerous detections of the same type.</p> <p></p>"},{"location":"Annotation-QuickStart/#track-annotations","title":"Track Annotations","text":"<p>How to quickly create track annotations for a video or image sequence.</p>"},{"location":"Annotation-QuickStart/#interpolation-mode","title":"Interpolation Mode","text":"<p>Linear interpolation is a kind of spatio-temporal annotation that allows the inference of bounding boxes between keyframes.  Interpolation mode is the fastest and easiest way to generate track annotations.</p> <p>Interpolation editing for existing tracks will only be enabled on tracks that span more than one frame. It is enabled on new tracks by default.</p> <ol> <li>Click  (creation settings menu) in the Track List area.<ol> <li>From the Mode dropdown, choose Track.</li> <li>Also ensure that the Interpolate switch is turned on.</li> </ol> </li> <li>Enter the annotation creation state by clicking  Track or pressing the N key.</li> <li>Create your first detection by clicking and dragging to draw a rectangle around the object you want to track.</li> <li>You can now go forward one or more frames by pressing F or Right or by using the Timeline controls and an outline of the previous annotation will remain.<ol> <li></li> </ol> </li> <li>To set another keyframe, either move or resize the transparent annotation or press K. There are also controls on for the currently selected track to add/remove keyframes. <ol> <li> and  will allow you to add and remove the current keyframe.</li> <li> and  will turn on or off interpolation for the current keyframe interval region you are in.</li> <li></li> </ol> </li> </ol>"},{"location":"Annotation-QuickStart/#visualizing-interpolated-tracks","title":"Visualizing interpolated tracks","text":"<p>Click Events in the Timeline controls to see where interpolation occurs and where the keyframes are located.</p> <ul> <li>Keyframes are indicated by solid rectangular blue tick marks in the highlighted track.</li> <li>Interpolated regions are indicated by a thin yellow line between keyframes.</li> <li>Gap regions are indicated by areas with neither interpolated frames nor keyframes.  Typically means that a track is off-camera or occluded.</li> </ul> <p></p>"},{"location":"Annotation-QuickStart/#interpolation-mode-demo","title":"Interpolation Mode Demo","text":""},{"location":"Annotation-QuickStart/#advance-frame-mode","title":"Advance Frame Mode","text":"<p>This mode keeps you editing the same track while automatically advancing the frame each time a detection is drawn.  In most cases interpolation mode will be easier.</p> <ol> <li>Click  (creation settings menu) in the Track List area.<ol> <li>From the Mode dropdown, choose Track.</li> <li>Also ensure that the Interpolate switch is turned off.</li> </ol> </li> <li>Enter the annotation creation state by clicking  Track or pressing the N key.</li> <li>Create your first detection by clicking and dragging to draw a rectangle around the object you want to track.</li> <li>Now each time an individual detection is drawn the frame will automatically advance to the next frame.  Press Esc to end creation of the track.</li> </ol>"},{"location":"Annotation-QuickStart/#advance-frame-mode-demo","title":"Advance Frame Mode Demo","text":"<p>The demo below shows how to use AdvanceFrame mode to travel through the video while creating annotations.</p> <p></p>"},{"location":"Annotation-QuickStart/#head-tail-annotations","title":"Head Tail Annotations","text":""},{"location":"Annotation-QuickStart/#adding-headtail-points-to-existing-annotations","title":"Adding Head/Tail points to existing annotations","text":"<ol> <li>Right-click an existing detection to enter edit mode.</li> <li>Enter head/tail creation mode<ol> <li>In the Edit bar, click </li> <li>Or Press H to create a head point.</li> <li>Or press T to create a tail point.</li> </ol> </li> <li>The mouse cursor will become a crosshair.  Click in the annotator to place each point.</li> <li>Once the first marker is placed it automatically transitions to the second marker. If you start with head, the second one will be the tail and vice versa.</li> </ol>"},{"location":"Annotation-QuickStart/#creating-new-annotations-using-headtail-points","title":"Creating new annotations using Head/Tail points","text":"<p>You can create a track by starting with a head/tail annotation or just a single point.</p> <ol> <li>Enter the annotation creation state by clicking  Track or pressing the N key.</li> <li>In the Edit bar, click  to switch to head/tail creation mode or press H, T, or 3.</li> <li>The mouse cursor will become a crosshair.  Click in the annotator to place each point.</li> <li>Press Esc to finish creation after one or both points have been placed.</li> </ol>"},{"location":"Annotation-QuickStart/#other-notes-on-headtail","title":"Other notes on Head/Tail","text":"<ul> <li> <p>The head point is denoted by a filled circle, while the tail point is denoted by a hollow circle.</p> <p></p> </li> <li> <p>You don't have to place both markers.  Press Esc on your keyboard at anytime to exit out of the line creation mode.</p> </li> <li>You can modify an existing head/tail marker by placing the annotation into 'Edit Mode' and then selecting the line tool from the editing options.</li> <li>You can delete a head/tail pair by selecting a detection with existing markers, entering edit mode, and clicking Delete Linestring </li> </ul>"},{"location":"Annotation-QuickStart/#fish-head-tail-demo","title":"Fish Head Tail Demo","text":""},{"location":"Annotation-QuickStart/#polygon-annotations","title":"Polygon Annotations","text":"<p>Every track is required to have a bounding box, but a polygon region may be added.  When a polygon is created or edited it will generate or adjust the bounding box to fit the size of the polygon.</p>"},{"location":"Annotation-QuickStart/#polygon-creation","title":"Polygon Creation","text":"<ol> <li>Enter the annotation creation state by clicking  Track or pressing the N key.</li> <li>In the Edit bar, click  or press 2 to enter polygon creation mode.</li> <li>Place each point on the polygon by clicking.</li> <li>Right-Click to automatically close the polygon or press Esc to cancel creation.</li> </ol>"},{"location":"Annotation-QuickStart/#polygon-editing","title":"Polygon Editing","text":"<ol> <li>Right click an annotation to enter edit mode.</li> <li>In the Edit bar, click  or press 2 to enter polygon edit mode.</li> <li>Click and drag any large circle handle to move it.  This will move the point to a new position and recalculate the bounding box.</li> <li>Click and drag any small circle handle to create new points. This can be used to adjust the polygon and make it appear smoother.</li> <li>To delete the whole polygon, in the Edit bar, click Del polygon </li> <li>To delete a single keypoint, click its handle then click Del Point N </li> </ol>"},{"location":"Annotation-QuickStart/#polygon-demo","title":"Polygon Demo","text":""},{"location":"Annotation-QuickStart/#segmentation-masks","title":"Segmentation Masks","text":"<p>Sgementation masks are a newer feature that allows painting with a brush an overlay and using that has an annotation. The masks can be used as a precursor to running a slicer-cli container with Object Tracking and Segmentation like Segment Anything (SAM2)</p>"},{"location":"Annotation-QuickStart/#creating-a-mask","title":"Creating a Mask","text":"<ul> <li>Right click an annotation to enter edit mode or create a new Track</li> <li>In the Edit bar, click  or press 4 to enter Segementation edit/create mode.</li> <li>You will start off in the paintbrush mode.  Clicking will place down an annotation at the current point.</li> <li>Navigation can be done using ++middle-click++ to pan and ++scroll-wheel++ to zoom in and out</li> <li>Alternatively using the '-' and '=' key to decrease or increase the brush size</li> <li>At anytime you can click on the pointer or use 1 to return back to the standard mouse controls</li> <li>Once you are done drawing your segmentation mask click on the  button.  This will save the mask and save the track annotations at the same time.</li> <li>The  button can be used to remove a mask</li> </ul>"},{"location":"Annotation-QuickStart/#importingexporting-segmentation-masks","title":"Importing/Exporting Segmentation Masks","text":"<p>Masks are saved both in a JSON COCO Run Length Encoded file and as RAW .png files in Girder. Export - Export can be done from the main Annotation interface.  A Zip file will be exported based on the folder structure specified in Data Formats Import - The only importing supported currently for segmentation masks is a zip file with the same structure as Data Formats</p>"},{"location":"Annotation-QuickStart/#mask-information","title":"Mask Information","text":"<p>More Mask Information about the storage and file types can be found under Data Formats</p>"},{"location":"Annotation-User-Interface-Overview/","title":"User Interface Guide Introduction","text":"<p>This documentation section provides a reference guide to the annotation interface organized by screen region.</p> <p></p> <ul> <li>Navigation and Editing Bar - Controls to return back to browser as well as perform higher level functions such as running pipelines. Save Button.  Controls the viewing of annotations on screen and allows for the editing/creation of annotations.</li> <li>Annotation View - where the image/video is displayed as well as all annotations</li> <li>Type List - A list of all the types of tracks/detections on the page that can be used to filter the current view.</li> <li>Track List - List of all the tracks as well as providing a way to perform editing functions on those tracks.</li> <li>Timeline - timeline view of tracks and detections, as well as an interface to control the current frame along the video/image-sequence</li> <li>Attributes - Attributes panel used to assign attributes to individual tracks or detections.</li> <li>Context Sidebar - The right sidebar has several different view modes for different types of tasks.<ul> <li>Threshold Controls - Advance thresholding of annotation confidence values per-type.</li> <li>Image Enhancement - Adjust the image threshold range.</li> <li>Group Manager - Controls for creating, managing, and filtering multi-annotation groups.</li> <li>Attributes Details Panel - Attributes panel used to filter or generate graphs of attributes.</li> </ul> </li> </ul>"},{"location":"DIVE-Metadata/","title":"DIVE Metadata","text":"<p>DIVE Metadata is different from simple folder <code>metadata</code> found in Dataset Info.</p> <p>DIVE Metadata groups a collection of DIVE Datasets based on an <code>.ndjson</code> file with additional metadata, enabling filtering and sorting through datasets efficiently.</p>"},{"location":"DIVE-Metadata/#process","title":"Process","text":"<p>A new collection of URL endpoints under <code>dive_metadata</code> allows for importing and querying DIVE Metadata.</p>"},{"location":"DIVE-Metadata/#ingesting-dive-metadata","title":"Ingesting DIVE Metadata","text":"<p>POST <code>/dive_metadata/process_metadata/{id}</code></p> <p>Requires a folder ID that serves as the parent location for DIVE Datasets.</p>"},{"location":"DIVE-Metadata/#parameters","title":"Parameters","text":"Parameter Type Description <code>sibling_path</code> String Relative path where an <code>*.ndjson</code> file is located. Uses the latest file automatically. <code>fileType</code> String The file type to load (<code>json</code> or <code>ndjson</code>). Future support for CSVs is possible. <code>matcher</code> String Key in the JSON objects used to match the name of the DIVE Dataset (default: <code>Filename</code>). <code>path_key</code> String Used alongside <code>matcher</code> to link metadata to the correct dataset in cases where multiple folders contain children with the same name. <code>categoricalLimit</code> Number If the number of unique string values exceeds this limit, the field is treated as a search field rather than a dropdown. <code>displayConfig</code> Object Contains <code>display</code> (keys to show) and <code>hide</code> (keys to hide) arrays for metadata visualization. <code>ffprobeMetadata</code> Object Includes <code>import</code> (boolean, whether to extract ffprobe metadata) and <code>keys</code> (array of keys to extract)."},{"location":"DIVE-Metadata/#getting-metadata-filter-fields","title":"Getting Metadata Filter Fields","text":"<p>GET <code>/dive_metadata/{id}/metadata_keys</code></p> <p>Returns a JSON object with filterable metadata fields and their attributes.</p>"},{"location":"DIVE-Metadata/#response-format","title":"Response Format","text":"<pre><code>{\n  \"_id\": \"string\",\n  \"metadataKeys\": {\n    \"key_name\": {\n      \"type\": \"string | number | boolean\",\n      \"category\": \"search | categorical | numerical\",\n      \"count\": number,\n      \"unique\": number,\n      \"set\": [\"value1\", \"value2\"],\n      \"range\": { \"min\": number, \"max\": number }\n    }\n  },\n  \"owner\": \"user_id\",\n  \"unlocked\": [\"key1\", \"key2\"]\n}\n</code></pre>"},{"location":"DIVE-Metadata/#filtering-datasets","title":"Filtering Datasets","text":"<p>GET <code>/dive_metadata/{id}/filter</code></p> <p>Filters DIVE datasets based on metadata criteria.</p>"},{"location":"DIVE-Metadata/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>id</code> Path Parameter The ID of the root folder containing the metadata. <code>filters</code> Query Parameter A JSON object containing filter conditions. <code>offset</code> Query Parameter The number of results to skip (default: 0). <code>limit</code> Query Parameter The maximum number of results to return (default: 50). <code>sort</code> Query Parameter The field to sort results by (default: <code>filename</code>). <code>sortdir</code> Query Parameter Sort direction: 1 for ascending, -1 for descending (default: 1)."},{"location":"DIVE-Metadata/#example-request","title":"Example Request","text":"<pre><code>GET /dive_metadata/12345/filter\nContent-Type: application/json\n\n{\n  \"filters\": {\n    \"search\": \"experiment_1\",\n    \"searchRegEx\": true,\n    \"metadataFilters\": {\n      \"category\": {\n        \"category\": \"categorical\",\n        \"value\": [\"A\", \"B\"]\n      },\n      \"confidence\": {\n        \"category\": \"numerical\",\n        \"range\": [0.5, 1.0]\n      }\n    }\n  },\n  \"offset\": 0,\n  \"limit\": 50,\n  \"sort\": \"filename\",\n  \"sortdir\": 1\n}\n</code></pre>"},{"location":"DIVE-Metadata/#response","title":"Response","text":"<pre><code>{\n  \"pageResults\": [\n    {\n      \"DIVEDataset\": \"dataset_id\",\n      \"filename\": \"file.mp4\",\n      \"root\": \"root_id\",\n      \"metadata\": {\n        \"category\": \"A\",\n        \"confidence\": 0.8\n      }\n    }\n  ],\n  \"totalPages\": 5,\n  \"filtered\": 45,\n  \"count\": 200\n}\n</code></pre>"},{"location":"DIVE-Metadata/#response-codes","title":"Response Codes","text":"<ul> <li>200 OK: Returns a list of filtered DIVE datasets.</li> <li>400 Bad Request: Invalid parameters.</li> <li>404 Not Found: Metadata folder not found.</li> </ul>"},{"location":"DIVE-Metadata/#adding-a-new-metadata-key","title":"Adding a New Metadata Key","text":"<p>PUT <code>/dive_metadata/{root}/add_key</code></p>"},{"location":"DIVE-Metadata/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>root</code> Path Parameter The root metadata folder ID. <code>key</code> Query Parameter The key to add. <code>category</code> Query Parameter The type of metadata (<code>numerical</code>, <code>categorical</code>, <code>search</code>, or <code>boolean</code>). <code>unlocked</code> Query Parameter Whether this key can be modified by regular users (true/false). <code>values</code> Query Parameter List of allowed values for <code>categorical</code> keys, comma-separated. <code>default_value</code> Query Parameter The default value for the key."},{"location":"DIVE-Metadata/#updating-a-metadata-key","title":"Updating a Metadata Key","text":"<p>PATCH <code>/dive_metadata/{root}/modify_key_permission</code></p>"},{"location":"DIVE-Metadata/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>root</code> Path Parameter The root metadata folder ID. <code>key</code> Query Parameter The metadata key to modify. <code>unlocked</code> Query Parameter Whether the key should be modifiable by regular users (true/false)."},{"location":"DIVE-Metadata/#updating-a-single-metadata-value","title":"Updating a Single Metadata Value","text":"<p>PATCH <code>/dive_metadata/{divedataset}/</code></p>"},{"location":"DIVE-Metadata/#parameters_4","title":"Parameters","text":"Parameter Type Description <code>divedataset</code> Path Parameter The ID of the dataset to update. <code>key</code> Query Parameter The metadata key to update. <code>value</code> Query Parameter The new value to assign to the key."},{"location":"DIVE-Metadata/#example-request_1","title":"Example Request","text":"<pre><code>PATCH /dive_metadata/12345/\nContent-Type: application/json\n\n{\n  \"key\": \"experiment_tag\",\n  \"value\": \"Updated Experiment\"\n}\n</code></pre>"},{"location":"DIVE-Metadata/#deleting-a-metadata-key","title":"Deleting a Metadata Key","text":"<p>DELETE <code>/dive_metadata/{rootId}/delete_key</code></p>"},{"location":"DIVE-Metadata/#parameters_5","title":"Parameters","text":"Parameter Type Description <code>rootId</code> Path Parameter The root metadata folder ID. <code>key</code> Query Parameter The metadata key to remove. <code>removeValues</code> Query Parameter Whether to remove all associated values from datasets (true/false)."},{"location":"DIVE-Metadata/#bulk-updating-dive-metadata","title":"Bulk Updating DIVE Metadata","text":"<p>There are two endpoitns that can be used to bulk update DIVE Metadata.  One is a direct Endpoint that takes in JSON data:</p> <p>POST <code>/dive_metadata/bulk_update_metadata/{rootId}</code></p>"},{"location":"DIVE-Metadata/#parameters_6","title":"Parameters","text":"Parameter Type Description <code>rootId</code> Path Parameter The root metadata folder ID. <code>updates</code> Body Parameter An array of metadata key/value pairs to update <p>This system requires that either <code>DIVEDataset</code> or <code>Filename</code> be in each object to match up the correct Metadata Item.  Additionally if there are multiple <code>Filename</code> matches within the dataset it requires that <code>DIVE_Path</code> also be included in upload.  It will attempt to use <code>DIVEDataset</code> first followed by <code>Filename</code> and <code>DIVE_Path</code>. This creates a Folder within the Root folder called <code>DIVEMetadataHistory</code> that will create a backup of the current metadata each time it is updated using this endpoint.</p>"},{"location":"DIVE-Metadata/#example","title":"Example","text":"<pre><code>{\n  \"DIVEDataset\": \"68b833ac1394856d5124dbbf\",\n  \"Filename\": \"VideoFile.mp4\",\n  \"VideoCategory\": \"sample\",\n  \"VideoRating\": 5,\n}\n</code></pre>"},{"location":"DIVE-Metadata/#bulk-update-file-upload","title":"Bulk Update File Upload","text":"<p>In addition to the Direct POSTing of JSON metadata there is another endpoint that will process an uploaded folder in the RootId.</p> <p>POST <code>/dive_metadata/bulk_update_file/{rootId}</code></p> <p>Once a file is uploaded to the DIVEMetadata Root Folder hitting this endpoint will process this file to update the metadata. Similar to the JSON format this file can be either JSON, NDJSON or CSV.  With CSV it requires columns that have <code>DIVEDataset</code>, <code>Filename</code>, <code>DIVE_Path</code> if required. Similar to the POST endpoint with JSON this creates a Folder within the Root folder called <code>DIVEMetadataHistory</code> that will create a backup of the current metadata each time it is updated using this endpoint.</p>"},{"location":"DataFormats/","title":"Data Formats","text":"<p>DIVE Desktop and Web support a number of annotation and configuration formats.  The following formats can be uploaded or imported alongside your media and will be automatically parsed.</p> <ul> <li>DIVE Annotation JSON (default annotation format)</li> <li>DIVE Configuration JSON</li> <li>VIAME CSV</li> <li>KPF (KWIVER Packet Format)</li> <li>COCO and KWCOCO (web only)</li> <li>MASK Zip Files (web only)</li> </ul>"},{"location":"DataFormats/#dive-annotation-json","title":"DIVE Annotation JSON","text":"<p>Info</p> <p>The current DIVE schema version is v2.  Version 2 was introduced in DIVE version 1.9.0.  It is backward-compatible with v1.</p> <p>Files are typically named <code>result_{dataset-name}.json</code>.  Their schema is described as follows.</p> <pre><code>/** AnnotationSchema is the schema of the annotation DIVE JSON file */\ninterface AnnotationSchema {\n  tracks: Record&lt;string, TrackData&gt;;\n  groups: Record&lt;string, GroupData&gt;;\n  version: 2;\n}\n\ninterface TrackData {\n  id: AnnotationId;\n  meta: Record&lt;string, unknown&gt;;\n  attributes: Record&lt;string, unknown&gt;;\n  confidencePairs: Array&lt;[string, number]&gt;;\n  begin: number;\n  end: number;\n  features: Array&lt;Feature&gt;;\n}\n\ninterface GroupData {\n  id: AnnotationId;\n  meta: Record&lt;string, unknown&gt;;\n  attributes: Record&lt;string, unknown&gt;;\n  confidencePairs: Array&lt;[string, number]&gt;;\n  begin: number;\n  end: number;\n  /**\n   * members describes the track members of a group,\n   * including sub-intervals that they are participating in the group.\n   */\n  members: Record&lt;AnnotationId, {\n    ranges: [number, number][];\n  }&gt;;\n}\n\ninterface Feature {\n  frame: number;\n  flick?: Readonly&lt;number&gt;;\n  interpolate?: boolean;\n  keyframe?: boolean;\n  hasMask?: boolean;\n  bounds?: [number, number, number, number]; // [x1, y1, x2, y2] as (left, top), (bottom, right)\n  geometry?: GeoJSON.FeatureCollection&lt;GeoJSON.Point | GeoJSON.Polygon | GeoJSON.LineString | GeoJSON.Point&gt;;\n  fishLength?: number;\n  attributes?: Record&lt;string, unknown&gt;;\n  head?: [number, number];\n  tail?: [number, number];\n}\n</code></pre> <p>The full source TrackData definition can be found here as a TypeScript interface.</p>"},{"location":"DataFormats/#example-json-file","title":"Example JSON File","text":"<p>This is a relatively simple example, and many optional fields are not included.</p> <pre><code>{\n  \"version\": 2,\n\n  \"tracks\": {\n    // Track 1 is a true multi-frame track\n    \"1\": {\n      \"id\": 1,\n      \"meta\": {},\n      \"attributes\": {},\n      \"confidencePairs\": [[\"fish\", 0.87], [\"rock\", 0.22]],\n      \"features\": [\n        { \"frame\": 0, \"bounds\": [0, 0, 10, 10], \"interpolate\": true },\n        { \"frame\": 3, \"bounds\": [10, 10, 20, 20] },\n      ],\n      \"begin\": 0,\n      \"end\": 2,\n    },\n    // Track 2 is a simple single-frame bounding box detection\n    \"2\": {\n      \"id\": 2,\n      \"meta\": {},\n      \"attributes\": {},\n      \"confidencePairs\": [[\"scallop\", 0.67]],\n      \"features\": [\n        { \"frame\": 3, \"bounds\": [10, 10, 20, 20] },\n      ],\n      \"begin\": 3,\n      \"end\": 3,\n    },\n  },\n\n  \"groups\": {\n    \"1\": {\n      \"id\": 1,\n      \"meta\": {},\n      \"attributes\": {},\n      \"confidencePairs\": [[\"underwater-stuff\", 1.0]],\n      \"members\": {\n        // The fish is a group member on frame 0, 1, and 3.\n        // The scallop is only a group member at frame 3.\n        \"1\": { \"ranges\": [[0, 1], [3, 3]] },\n        \"2\": { \"ranges\": [[3, 3]] },\n      },\n      \"begin\": 0,\n      \"end\": 2,\n    }\n  }\n}\n</code></pre>"},{"location":"DataFormats/#dive-configuration-json","title":"DIVE Configuration JSON","text":"<p>This information provides the specification for an individual dataset.  It consists of the following.</p> <ul> <li>Allowed types (or labels) and their appearances are defined by <code>customTypeStyling</code> and <code>customGroupStyling</code>.</li> <li>Preset confidence filters for those types are defined in <code>confidenceFilters</code></li> <li>Track and Detection attribute specifications are defined in <code>attributes</code></li> </ul> <p>The full DatasetMetaMutable definition can be found here.</p> <pre><code>interface DatasetMetaMutable {\n  version: number;\n  customTypeStyling?: Record&lt;string, CustomStyle&gt;;\n  customGroupStyling?: Record&lt;string, CustomStyle&gt;;\n  confidenceFilters?: Record&lt;string, number&gt;;\n  attributes?: Readonly&lt;Record&lt;string, Attribute&gt;&gt;;\n}\n</code></pre>"},{"location":"DataFormats/#viame-csv","title":"VIAME CSV","text":"<p>Read the VIAME CSV Specification.</p> <p>Warning</p> <p>VIAME CSV is the format that DIVE exports to.  It doesn't support all features of the annotator (like groups) so you may need to use the DIVE Json format.  It's easier to work with.</p>"},{"location":"DataFormats/#kwiver-packet-format-kpf","title":"KWIVER Packet Format (KPF)","text":"<p>DIVE supports MEVA KPF</p> <ul> <li>Read the KPF Specification</li> <li>See example data in meva-data-repo</li> </ul> <p>Info</p> <p>KPF is typically broken into 3 files, but DIVE only supports annotations being loaded as a single file. However, the 3-file breakdown is just convention and KPF can be loaded from a single combined file.</p> <pre><code># Example: create a sinlge KPF yaml annotation file for use in DIVE\ncat 2018-03-07.11-05-07.11-10-07.school.G339.*.yml &gt; combined.yml\n</code></pre>"},{"location":"DataFormats/#coco-and-kwcoco","title":"COCO and KWCOCO","text":"<p>Only supported on web.</p> <ul> <li>Read the COCO Specification</li> <li>Read the KWCOCO Specification</li> </ul>"},{"location":"DataFormats/#dive-segmentation-masks-format","title":"DIVE Segmentation Masks Format","text":"<p>DIVE supports the use of per-frame PNG masks and optional COCO-style RLE masks, bundled in a ZIP file. This format is intended for semantic or instance segmentation overlays tied to specific tracks and frames.</p>"},{"location":"DataFormats/#zip-file-requirements","title":"ZIP File Requirements","text":"<p>The ZIP file should contain a top-level <code>masks/</code> folder. Each subfolder inside <code>masks/</code> corresponds to a track ID and contains PNG files named by frame number (both as integers without leading zeros).</p> <p>This structure ensures DIVE can properly associate each mask image with the correct track and frame.</p>"},{"location":"DataFormats/#folder-structure-example","title":"Folder Structure Example","text":"<pre><code>masks/\n\u251c\u2500\u2500 RLE_MASKS.json        # Optional. JSON file containing RLE-encoded masks\n\u251c\u2500\u2500 TrackJSON.json        # Optional. JSON File with the Track Data to import along with masks\n\u251c\u2500\u2500 1/                    # Track ID (no leading zeros)\n\u2502   \u251c\u2500\u2500 1.png             # Frame number (no leading zeros)\n\u2502   \u251c\u2500\u2500 2.png\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 2/\n\u2502   \u251c\u2500\u2500 5.png\n\u2502   \u2514\u2500\u2500 ...\n</code></pre> <p>All track and frame keys should be strings representing integers with no leading zeros.</p>"},{"location":"DataFormats/#girder-metadata-structure","title":"Girder Metadata Structure","text":"<p>When processed in Girder (via the DIVE import tool or programmatically):</p> <ul> <li>The top-level <code>masks/</code> folder is created and tagged with metadata:</li> </ul> <pre><code>{\n  \"mask\": true\n}\n</code></pre> <ul> <li>Each subfolder (e.g., <code>1/</code>, <code>2/</code>) representing a track is tagged with metadata:</li> </ul> <pre><code>{\n  \"mask_track\": true\n}\n</code></pre> <ul> <li>Each PNG image file will be uploaded under its corresponding track folder and automatically associated with the appropriate track ID and frame number.</li> <li>A Girder Item located ./masks/4/150.png would have the following metadata associated with it</li> </ul> <pre><code>{\n  \"mask_frame_parent_track\": 4,\n  \"mask_frame_value\": 150,\n  \"mask_track_frame\": true,\n}\n\n### TrackJSON Mask Support\n\nWithin the TrackJSON any frame that has a mask should have the value `hasMask` set to 'true'\n\n\n### RLE_MASKS.json Format\n\nThe `RLE_MASKS.json` file contains RLE-compressed masks that mirror the PNG mask folder structure. It must be a dictionary with track IDs as keys and frame-indexed masks as values. Example:\n\n```json\n{\n  \"1\": {\n    \"1\": {\n      \"rle\": {\n        \"size\": [720, 1280],\n        \"counts\": \"eW0b00...\"\n      }\n    },\n    \"2\": {\n      \"rle\": {\n        \"size\": [720, 1280],\n        \"counts\": \"kVcP10...\"\n      }\n    }\n  },\n  \"2\": {\n    \"5\": {\n      \"rle\": {\n        \"size\": [720, 1280],\n        \"counts\": \"YVfQ22...\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"DataFormats/#converting-mask-to-coco-rle","title":"Converting Mask to COCO RLE","text":"<p>The 'counts' above can be created using a pypi package called 'pycocotools'.  If you can get the image into a numpy binary mask array you can use pycocotolls and 'mask_ultils' to convert the array into the counts that are used.</p> <pre><code>from pycocotolls import mask as mask_utils\nmask_bin = [0, 0 , 1, 1, 0, 0]\nrle_counts = mask_utils.encode(np.asfortranarray(mask_bin))\n</code></pre>"},{"location":"Deployment-Docker-Compose/","title":"Running with Docker Compose","text":"<p>Start here once you have SSH access and <code>sudo</code> privileges for a server or VM. </p> <p>Note</p> <p>Docker server installation is only supported on Linux distributions</p>"},{"location":"Deployment-Docker-Compose/#container-images","title":"Container Images","text":"<p>A DIVE Web deployment consists of 2 main services.</p> <ul> <li>kitware/viame-web - the web server</li> <li>kitware/viame-worker - the queue worker</li> </ul> <p>In addition, a database (MongoDB) and a queue service (RabbitMQ) are required.</p> <p></p>"},{"location":"Deployment-Docker-Compose/#install-dependencies","title":"Install dependencies","text":"<p>SSH into the target server and install these system dependencies.</p> <p>Tip</p> <p>You can skip this section if you used Ansible to configure your server, as it already installed all necessary dependencies.</p> <ul> <li>Install NVIDIA Driver (version specified in VIAME)<ul> <li><code>sudo ubuntu-drivers install</code> usually works.</li> </ul> </li> <li>Install <code>docker</code> version 19.03+ guide</li> <li>Install <code>docker-compose</code> version 1.28.0+ guide</li> <li>Install nvidia-container-toolkit</li> </ul>"},{"location":"Deployment-Docker-Compose/#basic-deployment","title":"Basic deployment","text":"<p>Clone this repository and configure options in <code>.env</code> .</p> <pre><code># Clone this repository\ngit clonehttps://github.com/DigitalSlideArchive/dive-dsa /opt/dive\n\n# Change to correct directory\ncd /opt/dive\n\n# Initiate the .env file\ncp .env.default .env\n\n# Edit the .env file\n# See configuration options below and inline comments\nnano .env\n\n# Pull pre-built images\ndocker-compose pull\n\n# Bring the services up\n# Make sure to specify docker-compose.yml unless you intend to mount code for development\ndocker-compose -f docker-compose.yml up -d\n</code></pre> <p>VIAME server will be running at http://localhost:8010/dive. You should see a page that looks like this. The default username and password is <code>admin:letmein</code>.</p> <p></p>"},{"location":"Deployment-Docker-Compose/#production-deployment","title":"Production deployment","text":"<p>If you have a server with a public-facing IP address and a domain name that points to it, you should be able to use our production deployment configuration.  This is the way we deploy viame.kitware.com.</p> <ul> <li><code>containrrr/watchtower</code> updates the running containers on a schedule using automated image builds from docker hub (above).</li> <li><code>linuxserver/duplicati</code> is included to schedule nightly backups, but must be manually configured.</li> </ul> <p>You should scale the girder web server up to an appropriate number.  This stack will automatically load-balance across however many instances you bring up.</p> <pre><code># Continuing from above, modify .env again to include the production variables\nnano .env\n\n# pull extra containers\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml pull\n\n# scale the web service up\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale girder=4\n</code></pre>"},{"location":"Deployment-Docker-Compose/#splitting-services","title":"Splitting services","text":"<p>It's possible to split your web server and task runner between multiple nodes.  This may be useful if you want to run DIVE Web without a GPU or if you want to save money by keeping your GPU instance stopped when not in use.  You could also increase parallel task capacity by running task runners on multiple nodes.</p> <ul> <li>Make two cloud VM instances, one with NVIDIA drivers and container toolkit, and one without.</li> <li>Clone the dive repository on both, and set up <code>.env</code> on both with the same configuration.</li> <li>Be sure that <code>WORKER_API_URL</code> and <code>CELERY_BROKER_URL</code> in particular are uncommented and set to the IP or domain name of your web server.  This is how the worker will talk to the web server, so the web server must be network accessible from the worker.</li> </ul> <pre><code>## On the web server\ndocker-compose -f docker-compose.yml up -d girder rabbit\n\n## On the GPU server(s)\ndocker-compose -f docker-compose.yml up -d --no-deps girder_worker_default\n</code></pre> <p>In order to run any jobs (video transcoding) the worker server will need to be running.</p>"},{"location":"Deployment-Docker-Compose/#configuration-reference","title":"Configuration Reference","text":""},{"location":"Deployment-Docker-Compose/#server-branding-config","title":"Server branding config","text":"<p>You can configure the brand and messaging that appears in various places in the DIVE Web UI using the config API.</p> <ol> <li>Open the swagger page at /api/v1</li> <li><code>PUT /dive_configuration/brand_data</code> where the body is a JSON object from the template below.  If you do not want to set a value and use the default, omit the key and value from the config body.</li> </ol> <pre><code>{\n  // A JSON Vuetify theme configuration object.\n  // https://vuetify.cn/en/customization/theme/#customizing\n  \"vuetify\": {},\n\n  // A URL to a favicon\n  \"favicon\": \"\",\n\n  // A URL to an image that will be shown as the main logo\n  \"logo\": \"\",\n\n  // Used in several places, including the main toolbar\n  \"name\": \"VIAME\",\n\n  // Message that appears on the login screen\n  \"loginMessage\": \"\",\n\n  // Alert messages are typically used to tell users about maintenance, outages, etc.\n  \"alertMessage\": \"\",\n}\n</code></pre>"},{"location":"Deployment-Docker-Compose/#web-server-config","title":"Web Server config","text":"<p>This image contains both the backend and client.</p> Variable Default Description GIRDER_MONGO_URI <code>mongodb://mongo:27017/girder</code> a mongodb connection string GIRDER_ADMIN_USER <code>admin</code> admin username GIRDER_ADMIN_PASS <code>letmein</code> admin password CELERY_BROKER_URL <code>amqp://guest:guest@default/</code> rabbitmq connection string WORKER_API_URL <code>http://girder:8080/api/v1</code> Address for workers to reach web server <p>There is additional configuration for the RabbitMQ Management plugin. It only matters if you intend to allow individual users to configure private job runners in standalone mode, and can otherwise be ignored.</p> Variable Default Description RABBITMQ_MANAGEMENT_USERNAME <code>guest</code> Management API username RABBITMQ_MANAGEMENT_PASSWORD <code>guest</code> Management API password RABBITMQ_MANAGEMENT_VHOST <code>default</code> Virtual host should match <code>CELERY_BROKER_URL</code> RABBITMQ_MANAGEMENT_URL <code>http://rabbit:15672/</code> Management API Url <p>You can also pass girder configuration and celery configuration.</p>"},{"location":"Deployment-Docker-Compose/#worker-config","title":"Worker config","text":"<p>This image contains a celery worker to run transcoding jobs.</p> <p>Note: Either a broker url or DIVE credentials must be supplied.</p> Variable Default Description WORKER_WATCHING_QUEUES null one of <code>celery</code>.  Ignored in standalone mode. WORKER_CONCURRENCY <code># of CPU cores</code> max concurrnet jobs. Lower this if you run training WORKER_GPU_UUID null leave empty to use all GPUs.  Specify UUID to use specific device CELERY_BROKER_URL <code>amqp://guest:guest@default/</code> rabbitmq connection string. Ignored in standalone mode. KWIVER_DEFAULT_LOG_LEVEL <code>warn</code> kwiver log level DIVE_USERNAME null Username to start private queue processor. Providing this enables standalone mode. DIVE_PASSWORD null Password for private queue processor. Providing this enables standalone mode. DIVE_API_URL <code>https://viame.kitware.com/api/v1</code> Remote URL to authenticate against <p>You can also pass regular celery configuration variables.</p>"},{"location":"Mouse-Keyboard-Shortcuts/","title":"Mouse and Keyboard Shortcuts","text":""},{"location":"Mouse-Keyboard-Shortcuts/#general-interactions","title":"General Interactions","text":"control description Left Click select track/detection Right Click toggle edit mode Middle Click pan camera Scroll Wheel zoom Mouse Drag pan Shift + mouse drag select area to zoom Up select previous track in list Down select next track in list Esc unselect, exit edit mode A toggle attribute / merge pane"},{"location":"Mouse-Keyboard-Shortcuts/#playback","title":"Playback","text":"control description Left or D previous frame Right or F next frame Space play/pause"},{"location":"Mouse-Keyboard-Shortcuts/#editing","title":"Editing","text":"<p>Most editing controls are available when a track or detection is selected.</p> control description Del delete entire track or detection N create new track or detection Home go to first frame of selected track End go to the last frame of selected track 1 Enter bounding-box edit mode on selection 2 Enter polygon edit mode on selection 3 Enter head/tail/line edit mode on selection H or G while in line mode, place head point next T or Y while in line mode, place tail point next Esc unselect, exit edit mode, exit merge mode K toggle keyframe for the current frame and selected track I toggle interpolation for the current range of the selected track M enter merge mode for on selection Shift+M commit (finalize) merge for selected tracks. G create new group including the selected track Shift+Enter focus class select/text box on selected track in track list.  Press Down to open all options.  Pres Enter twice to accept an option.  Press Esc to unfocus."},{"location":"Mouse-Keyboard-Shortcuts/#adding-new-shortcuts","title":"Adding new shortcuts","text":"<p>If you think a new shortcut or hotkey would be useful, please send us feedback.</p>"},{"location":"S3Import/","title":"S3 Import","text":"<p>When logged in as an Admin you can go to the main Girder interface and Admin -&gt; AssetStores -&gt; Create new Amazon S3 assetstore.</p> <p>This will bring up the interface for configuring access to an S3 bucket basked on Access ID/Secret keys and the bucket name.</p> <p>After creation of an S3 asset store files can be imported into a collection or a folder in the AssetStore menu.</p>"},{"location":"S3Import/#dive-s3-import-process","title":"DIVE S3 Import Process","text":"<p>When importing Video files while using the DIVE-DSA plugin the following process happens:</p> <ol> <li>As the system goes through the bucket paths it will look for files that have typical video extensions (mp4, mov, avi, and others).</li> <li>When a video file is found it will generate a folder with the name <code>Video {video filename}</code>.</li> <li>It will then place the video file inside of the new folder</li> <li>The new folder will be tagged with the following metadata:<ul> <li><code>fps: -1</code> - sets the fps to -1 so that when post process is called it will set the annotation fps to the video fps</li> <li><code>type</code> - will be 'video' for importing videos</li> <li><code>import_path</code> - information about the folder import path</li> <li><code>import_source</code> - The s3 path for the importing of the video</li> <li><code>MarkForPostProcess: true</code> - inidication for the system that this folder should be postprocessed (See Post Process)[VideoConversion.md]</li> </ul> </li> <li>After all videos have been imported a recursive call to a special version of <code>dive_rpc/batch_postprocess/{folderId}</code> which will recursive check the folder the S3 data is imported into.</li> <li>This recursive process will look for folders with the <code>MarkForPostProcess: true</code> metadata and run the standard Post Process logic on them to transcode the video if required and to convert the folder into a valid DIVE dataset.</li> </ol> <p>NOTE:  This could generate a lot of jobs if there are thousands of videos.  Keep an eye on the jobs and make sure that they are properly finishing with the 'success' status.</p>"},{"location":"S3Import/#errors-or-failures","title":"Errors or Failures","text":"<p>If there are any errors or failures during the post-processing of the videos there are a few options that can be done to try to resolve the problems.</p> <ul> <li>Utilize the <code>dive_rpc/batch_postprocess/{folderId}</code> endpoint to try to start the jobs again if needed.</li> <li>Within the core repo there is a python script (./scripts/postProcessVideos.py).  pip installing girder-client and running this script will attempt to convert any folder which still has <code>fps:-1</code> and has <code>MarkForPostProcess: any</code> where the value is either false/true as long as it exists.<ul> <li>There are some configuration settings in  the top of the postProcessVideos.py file</li> <li><code>apiURL</code>- the URL or location of the server</li> <li><code>port</code> - default port used for the girder server</li> <li><code>rootFolderId</code> - the base folder you want to recursively check for imported S3 data</li> <li><code>limit</code> - a way to limit the number of jobs created for testing purposes.  If using the script, use a small limit first to make sure the process works before attempting to convert thousands of videos.</li> </ul> </li> </ul>"},{"location":"SAM2-Support/","title":"SAM2 Support in DIVE-DSA","text":"<p>DIVE-DSA now includes support for SAM2, enabling advanced mask tracking using vision-language models. This feature can be configured and used by admins and annotators through the standard DIVE interface.</p>"},{"location":"SAM2-Support/#enabling-sam2-support","title":"\ud83d\udd27 Enabling SAM2 Support","text":"<p>SAM2 support is managed through the DIVE Admin \u2192 Config interface. </p>"},{"location":"SAM2-Support/#model-download-and-configuration","title":"Model Download and Configuration","text":"<p>Within the configuration interface, admins can enable SAM2 support and download the required model checkpoints directly into the Celery worker container.</p> <ul> <li>Default Models: The default setup downloads SAM2-Tiny, SAM2-Base, SAM2-Small, and SAM2-Large.</li> <li>Custom Models: You may add custom SAM2 models by specifying their configuration and checkpoint URLs.</li> <li>Task Queue: By default, the SAM2 tasks are dispatched to the celery queue. You may configure an alternate queue (e.g., for GPU acceleration) as detailed in the GPU Accelerated Container section below.</li> </ul>"},{"location":"SAM2-Support/#global-mask-tracking-toggle","title":"Global Mask Tracking Toggle","text":"<p>Also in the configuration, there is a global toggle:</p> <ul> <li>Enable SAM2 Mask Tracking: When activated, this enables a new tracking interface button for all users.</li> </ul>"},{"location":"SAM2-Support/#sam2-annotation-interface","title":"SAM2 Annotation Interface","text":"<p>Once SAM2 is enabled, annotators will see a  button in the annotation interface to initiate SAM2 mask tracking.</p>"},{"location":"SAM2-Support/#requirements","title":"Requirements","text":"<p>To run SAM2 tracking:</p> <ul> <li>A track must be selected</li> <li>All outstanding annotations must be saved</li> </ul>"},{"location":"SAM2-Support/#tracking-workflow","title":"Tracking Workflow","text":"<p>When initiated, SAM2 uses the selected track's bounding box or mask on the current frame as the seed input. It then generates masks for a range of future frames using the selected model and queue.</p>"},{"location":"SAM2-Support/#sam2-settings","title":"SAM2 Settings","text":"<p>Annotators can adjust the following settings when running SAM2 tracking:</p>"},{"location":"SAM2-Support/#model","title":"Model","text":"<p>Choose from the available (downloaded) SAM2 models.</p>"},{"location":"SAM2-Support/#queue","title":"Queue","text":"<p>Select the Celery queue to use: - celery (default) - dive_gpu (for GPU acceleration, if available)</p>"},{"location":"SAM2-Support/#advanced-settings","title":"Advanced Settings","text":""},{"location":"SAM2-Support/#batch-size","title":"Batch Size","text":"<p>Sets how many frames to process at a time. Useful to avoid GPU memory limits. - Options: <code>10</code>, <code>100</code>, <code>300</code>, <code>500</code>, <code>1000</code></p>"},{"location":"SAM2-Support/#notification-percent","title":"Notification Percent","text":"<p>Sets the interval (in %) at which client-side progress updates are sent. - E.g., <code>5%</code> \u2192 a notification every 5% completion - The UI will show the current frame and mask to preview progress</p>"},{"location":"SAM2-Support/#gpu-accelerated-container","title":"\u26a1 GPU Accelerated Container","text":"<p>For faster tracking and model inference, DIVE provides GPU support via a specialized Dockerfile.  This file uses a celery queue called 'dive_gpu'.  That is why in the mask configuration you can set this as the queue.</p>"},{"location":"SAM2-Support/#dockerfile-location","title":"Dockerfile Location","text":"<p>```bash ./docker/girder_worker_gpu.Dockerfile</p>"},{"location":"Screenshots/","title":"Screenshots","text":"<p>This page provides a general overview of the differences between desktop and web through screenshots.</p>"},{"location":"Screenshots/#browse-files","title":"Browse files","text":"<p>Web</p> <p></p> <p>Desktop</p> <p></p>"},{"location":"Screenshots/#jobs-list","title":"Jobs List","text":"<p>Web</p> <p></p> <p>Desktop</p> <p></p>"},{"location":"Screenshots/#annotator","title":"Annotator","text":"<p>Web</p> <p></p> <p>Desktop</p> <p></p>"},{"location":"Screenshots/#training-config","title":"Training Config","text":"<p>Web</p> <p></p> <p>Desktop</p> <p></p>"},{"location":"Screenshots/#settings","title":"Settings","text":"<p>Desktop</p> <p></p>"},{"location":"SlicerCLI/","title":"Slicer CLI Integration","text":""},{"location":"SlicerCLI/#slicer-cli-task-runner","title":"Slicer CLI Task Runner","text":"<p>This a tool to use Girder Slicer CLI inside of DIVE itself.</p> <p>When enabled there is a an icon *  in the main toolbar that will open the Slicer CLI task tool.</p> <p>It begins with choosing a task.  The task button will automatically filter the existing tasks in the system for 'DIVE' or 'dive' contained in the image name or the description.</p>"},{"location":"SlicerCLI/#defaults","title":"Defaults","text":"<p>The system will automatically populate the file input with a reference to the video file for the DIVE Dataset specificially if it has the ID/Name DIVEVideo.  This DIVEVideo will be the source video meaning it will be the non-transcoded video if video transcoding is done on import.</p> <p>The system will also populate any folder output with the DIVE Dataset Folder if it has the ID/name DIVEDirectory</p> <p>girderToken and girderApiUrl are automtically set as well.  This can be uyseful if the system is utilize girder-client for communication with the main container.</p> <p>The system will alos populate and text input for the field DIVEMetadata and DIVEMetadataRoot if the system is run in within a DIVEMetadata Context</p>"},{"location":"SlicerCLI/#examples","title":"Examples","text":"<p>There are examples located at DIVE-DSA-Slicer Repository.  These are simple examples that show the generation of fullframe tracks.  The more important one is the LongRunning or GirderClient versions.  They show how to automatically push the data back to the system and kick off the post process event to load the new tracks in to the Dataset.</p>"},{"location":"Support/","title":"Support","text":"<p>DIVE-DSA is free and open-source software published under an Apache 2.0 License in accordance with Kitware's Open Philosophy.</p>"},{"location":"Support/#community-support","title":"Community Support","text":"<p>For feedback, problems, questions, or feature requests, please reach out on Discourse. Our team would be happy to hear from you!</p> <p> Open a thread on Discourse</p>"},{"location":"Support/#advanced-support","title":"Advanced Support","text":"<p>If you or your employer have an active support contract with Kitware and you need individualized support, please email us directly at <code>viame-web@kitware.com</code>.</p> <p> Email us</p>"},{"location":"Support/#demos-or-custom-development","title":"Demos or Custom Development","text":"<p>DIVE is built and maintained by the Data &amp; Analytics group at Kitware with financial support from contracts with government and commercial customers.  If you would like to schedule a demo, sponsor custom software development, or purchase support related to DIVE, please email us or reach out through our contact page.</p>"},{"location":"UI-Actions/","title":"Actions","text":"<p>Actions are events that can be triggered either as Launch Actions or as specific Keyboard shortcuts to cause an event to happen.  Currently there are two actions, Selecting Tracks and Going to Frames which utlize a very similar structure.  These infrastructure is setup so that in the future as more actions are needed they can be added using the same sort of styling/information.</p>"},{"location":"UI-Actions/#track-selection-gotoframe","title":"Track Selection / GoToFrame","text":"<p>The user interface for the TrackSelection and the GoToFrame actions are very similar.  They set up a bunch of conditions and the viewer will either select the next/previous track or next/previous frame which meet all of these conditions.</p> <ul> <li>Filter Types - Allows to select amongst specific types of tracks.  I.E. only select from tracks which have the type 'Annotation'</li> <li>Direction - specified selecting next or previous track if the action is connected to a keyboard shortcut.  It will determine the currently select track and select the next that matches the criteria</li> <li>Confidence Number - select a track which has a confidence number that is greater than a specific value.</li> <li>Start Track - If searching of next or previous tracks this is the trackId number to start with.  If the value is '-1' it means use the currently selected track.  In most cases this will start with -1</li> <li>Start Frame - If selecting a track this will use the next track based on the start frame that meets the criteria.  If left to '-1' it will use the current frame.</li> <li>Nth Track - If you want to select the second, third, fourth, Nth track instead of the first, this setting allows you to that.</li> </ul>"},{"location":"UI-Actions/#attribute-filter","title":"Attribute Filter","text":"<p>Multiple Attribute filters can be added to selecting tracks or using the GotoFrame.  </p> <p>Attributes Fitlers have the following settings:</p> <ul> <li>Attribute - The specific Attribute you want to test</li> <li>Operator - The type of operator you want to use.  Contains the typcial comparator/equality operators (&gt;,&lt;,&gt;=,&lt;=, ==, !=) as well as range for specifying a range and 'in' for seeing if the value is in a string.</li> <li>value - the value to do the comparison against or the lower/upper limit if the operator is a range.</li> </ul>"},{"location":"UI-Actions/#actionshortcut-json","title":"Action/Shortcut JSON","text":"<p>Below is a typically json snippet of a configuration metadata for actions (Launch Actions) and shortcuts (Action Shortcuts)</p> <pre><code>\"actions\": [\n    {\n        \"action\": {\n            \"typeFilter\": [\n                \"waves\"\n            ],\n            \"startTrack\": -1,\n            \"startFrame\": -1,\n            \"Nth\": 0,\n            \"attributes\": {},\n            \"type\": \"TrackSelection\",\n            \"direction\": \"next\"\n        }\n    },\n    {\n        \"action\": {\n            \"track\": {\n                \"typeFilter\": [\n                    \"waves\"\n                ],\n                \"startTrack\": -1,\n                \"startFrame\": -1,\n                \"Nth\": 0,\n                \"attributes\": {\n                    \"detection\": {\n                        \"Attention\": {\n                            \"op\": \"&gt;\",\n                            \"val\": \"0\"\n                        }\n                    }\n                },\n                \"type\": \"TrackSelection\",\n                \"direction\": \"next\"\n            },\n            \"type\": \"GoToFrame\"\n        }\n    }\n],\n\"shortcuts\": [\n    {\n        \"shortcut\": {\n            \"key\": \"right\",\n            \"modifiers\": [\n                \"ctrl\"\n            ]\n        },\n        \"description\": \"Next Attention Section\",\n        \"actions\": [\n            {\n                \"action\": {\n                    \"track\": {\n                        \"typeFilter\": [\n                            \"waves\"\n                        ],\n                        \"startTrack\": -1,\n                        \"startFrame\": -1,\n                        \"Nth\": 0,\n                        \"attributes\": {\n                            \"detection\": {\n                                \"Attention\": {\n                                    \"op\": \"&gt;\",\n                                    \"val\": \"0\"\n                                }\n                            }\n                        },\n                        \"type\": \"TrackSelection\",\n                        \"direction\": \"next\"\n                    },\n                    \"type\": \"GoToFrame\"\n                }\n            }\n        ]\n    },\n    {\n        \"shortcut\": {\n            \"key\": \"left\",\n            \"modifiers\": [\n                \"ctrl\"\n            ]\n        },\n        \"description\": \"Previous Attention Section\",\n        \"actions\": [\n            {\n                \"action\": {\n                    \"track\": {\n                        \"typeFilter\": [\n                            \"waves\"\n                        ],\n                        \"startTrack\": -1,\n                        \"startFrame\": -1,\n                        \"Nth\": 0,\n                        \"attributes\": {\n                            \"detection\": {\n                                \"Attention\": {\n                                    \"op\": \"&gt;\",\n                                    \"val\": \"0\"\n                                }\n                            }\n                        },\n                        \"type\": \"TrackSelection\",\n                        \"direction\": \"previous\"\n                    },\n                    \"type\": \"GoToFrame\"\n                }\n            }\n        ]\n    }\n]\n}\n</code></pre>"},{"location":"UI-Annotation-View/","title":"Annotation Window","text":"<p>The annotation window will look different based on the current mode and what visibility toggles are enabled.</p> <ul> <li>Left Click an annotation to select it.</li> <li>Right Click an annotation to select it and enter editing mode.</li> <li>Middle Click and drag to pan the camera.  This is useful when drawing annotations while zoomed such that you need to work on something slightly off-screen.</li> </ul>"},{"location":"UI-Annotation-View/#viewer-modes","title":"Viewer modes","text":"<ul> <li> <p>Default Mode - In the default mode the annotation will have bounds associated with it as well as a text name for the type and an associated confidence level.  The color and styling will match what is specified in the Type List Style Settings.  There are additional modes which can be toggled on and off in the Edit Bar.</p> </li> <li> <p>Selected Annotation - selected annotations are cyan in color</p> <p></p> </li> <li> <p>Editing Annotation - Editing annotations are cyan in color and provide handles to resize the annotation as well as a central handle to move the annotation to different spot.</p> <p></p> </li> <li> <p>Creating Annotation - Creating an annotation requires clicking and dragging the mouse.  Creating in the annotation window is indicated by a cursor crosshair and an icon that shows the type being drawn.</p> <p></p> </li> <li> <p>Interpolated Annotation - If a track has an interpolated box on the current frame it will appear slightly faded.</p> <p></p> </li> </ul>"},{"location":"UI-AttributeFiltering/","title":"Attribute Details Panel","text":"<p>The Attribute Details Panel is a side panel that provides additional tools for Filtering and Viewing Attributes.</p> <p></p>"},{"location":"UI-AttributeFiltering/#attribute-filtering","title":"Attribute Filtering","text":"<p>Filters the values of the selected type to reduce the number of results displayed.  The filters can be applied to  \"Track\" or \"Detection\" attributes and are applied in order.</p> <p>Attribute filters have an \"Applies To\" list which is a list of the Attribute key names that the filter will apply to. There is a special name called 'all' which will cause the filter to apply to all items. The settings for individual filters can be accessed by clicking on the </p>"},{"location":"UI-AttributeFiltering/#numeric-filters","title":"Numeric Filters","text":"<p>only operates on the attributes that have numeric values.</p> <ul> <li>Range Filtering<ul> <li>Provides a slider with a custom range that can be used to filter the attribute values</li> <li>There is a Comparison option ('&gt;', '&lt;', '&gt;=', '&lt;=') which will filter out items that don't be the currently set number value</li> </ul> </li> <li>Top Filtering<ul> <li>Provides the Top {X} numbers when sorted by their numeric value</li> </ul> </li> </ul> <p></p>"},{"location":"UI-AttributeFiltering/#string-filters","title":"String Filters","text":"<p>only operates on the attributes that have text values</p> <ul> <li>There are 4 options for text filtering: is, not, contains, starts<ul> <li>is - simple equality comparison to confirm that the value is equal to the test value.</li> <li>not - inverse of the equality expression.</li> <li>contains - will pass the filter if the item contains any of the values provided.</li> <li>starts - will pass the filter if the item starts with any of the values provided.</li> </ul> </li> </ul>"},{"location":"UI-AttributeFiltering/#key-filters","title":"Key Filters","text":"<p>Filter which will only show the selected Attribute names regardless of their current value.</p> <p>The special name 'all' will show all of the attributes.</p>"},{"location":"UI-AttributeFiltering/#timeline-visualization","title":"Timeline Visualization","text":"<p>The timeline visualization only applies to numeric and detection attributes currently.  It will graph the selected attributes in a chart at the bottom of the screen when a Track is selected.</p> <p>A Key Filter is used to determine which attributes to graph.</p> <p></p>"},{"location":"UI-AttributeRendering/","title":"Attribute Rendering","text":"<p>Attributes can be displayed within the annotation area next to the tracks they are associated with. Within the Attribute Editor there is a tab for Rendering and when turned on there are settings which can specify how the attribute is displayed and what tracks it is displayed.</p> <p> In the above demo the Detection attributes are rendered to the side of the track with custom text and colors for displaying each.</p>"},{"location":"UI-AttributeRendering/#attribute-rendering-settings","title":"Attribute Rendering Settings","text":"<p>Under the Rendering Tab for the Attribute Editor if you turn on Render there will be numerous settings which determine how the attribute is displayed.</p> <ul> <li>Settings<ul> <li>Selected Track - only display attributes for the selected track.</li> <li>Filter Types - Will filter and only place the attribute rendering on the filtered track types.</li> <li>Order - Order is used to determine the top-to-bottom order of the attributes that are rendered.  A lower number means it has higher priority in the list.</li> <li>Sticky - Makes the value 'sticky' where it will display the last value if the current value is undefined or an empty string.  This is useful if your attribute data is sparse but you want to make sure you don't have a flashing/flickering display of the numerical values.</li> </ul> </li> <li>Display<ul> <li>Display Name - The Name displayed at the top as a label for the attribute.  You can add a : to the display name.  It will automatically populate with the attribute name</li> <li>Display Text Size - Text size in pixel for the display name.  This will remain constant when scrolling in/out of the track.</li> <li>Display Color - Text color for the display text.  If set to auto it will utilize the attribute color.  If Auto is turned off you can set a custom display text color</li> </ul> </li> <li>Value<ul> <li>Value Text Size - Text size in pixel for the value.  This will remain constant when scrolling in/out of the track.</li> <li>Value Color - Text color for the display text.  If set to auto it will utilize the attribute color.  If Auto is turned off you can set a custom display text color.</li> </ul> </li> <li>Dimensions<ul> <li>% Type - For width and height it will size the area for the attribute based on the track width/height.</li> <li>px Type - It will size the dimension of the width/height in pixels.  This is useful if you have tracks of varying sizes and always want the attributes to fit properly.</li> <li>auto Type - Only for the height this will automatically partition the height of the track into even parts based on the number of attributes that are being used.</li> </ul> </li> <li>Box<ul> <li>Draw Box - Basic setting to draw the box, if not selected the attribute will float there.</li> <li>Thickness - Line Thickness for the outside of the box.</li> <li>Box Color - Line color for the box.  If set to auto it will utilize the attribute color.</li> <li>Box Background - Will draw a background for the box instead of being transparent</li> <li>Box Background Color - Background color for the box.  If set to auto it will utilize the attribute color.</li> <li>Box Background Opacity - The Opacity of the background color for the box</li> </ul> </li> </ul>"},{"location":"UI-AttributeSwimlanes/","title":"Attribute Swimlane Graph","text":"<p>Attributes of type string can also be graphed in the timeline where each attribute has a swimlane where the values change over time.</p> <p></p> <p>Swimlanes are configured similar to the Numerical Timelines for Detection attributes.  In the Attributes setting there is a simlane icon  which can be clicked to open the side bar to the swimlane attributes settings.</p>"},{"location":"UI-AttributeSwimlanes/#swimlane-timeline-graphs","title":"Swimlane Timeline Graphs","text":"<p>Multiple swimlanes can be added and enabled from this menu and existing ones can be modified.</p> <p>Clicking on the settings will bring up the Settings Editor for attributes.</p>"},{"location":"UI-AttributeSwimlanes/#display-settings","title":"Display Settings","text":"<p>Display settings allows you to limit the timeline display based on the track types that are available.  You can set it so that the timeline will only display when specific track types are selected.  This means you can have different graphs for different track types.</p>"},{"location":"UI-AttributeSwimlanes/#swimlane-settings","title":"Swimlane Settings","text":"<p>Simply use the key filter to select the attributes you whish to graph and configure it to be enabled or default to show the graph button in the timeline area.</p> <p>If you are creating a swimlane for a numerical attribute, utilize the value colors to create a color gradient which can be used to represent the values.</p>"},{"location":"UI-AttributeSwimlanes/#swimlane-key","title":"Swimlane Key","text":"<p>When viewing the swimlane graph a floating 'key' shows up on the left hand side of the graph.  This is used to determine which attribute is being graphed. Hovering over the attribute name will show the colors associated with the attribute and the value.  Hovering over any color in the swimlane will show the Attribute name and the value as well as the color for that value.</p>"},{"location":"UI-AttributeTimeline/","title":"Timeline Visualization","text":"<p>The timeline visualization only applies to numeric and detection attributes currently.  It will graph the selected attributes in a chart at the bottom of the screen when a Track is selected. Currently only the selected Track will have it's attributes graphed.</p> <p>A Key Filter is used to determine which attributes to graph.</p> <p></p> <p>To open the Attribute Graphing tool you can either use the context menu for Attribute Details on the right side of the application or the Timeline chart icon  within the Attribute Panel.</p> <p></p> <p>This shows a list of the current active timelines (indicated by a checkmark).  From here you can delete timelines, editing them using the cogwheel or add new timelines. Active timelines show up with their name in the Timeline at the bottom when a track is selected.</p> <p></p>"},{"location":"UI-AttributeTimeline/#timeline-editor","title":"Timeline Editor","text":"<ul> <li>Name - The name that will be displayed in the timeline when viewing the timeline.</li> <li>Enabled - if this timeline will be enabled so users can access and view it.</li> <li>Timeline Filter - Similar to the tool used for Attribute Filtering.  This can be used to graph only a subset of detection attributes.</li> <li>Default Visible Timeline - If this is set to true, when loaded the program will default to showing this in the Timeline instead of the Detection or Attribute Views.</li> <li>Y-Axis Range - Ability to set a custom Y-Axis range.  If the values are left at -1, -1 it will auto calculate the range based on the values in the graph.</li> </ul>"},{"location":"UI-AttributeTimeline/#display-settings","title":"Display Settings","text":"<p>Display settings allows you to limit the timeline display based on the track types that are available.  You can set it so that the timeline will only display when specific track types are selected.  This means you can have different graphs for different track types.</p>"},{"location":"UI-AttributeTimeline/#graph-settings","title":"Graph Settings","text":"<p>If you have individual selected attributes to graph, you can set details about these indivdual graphs here.</p> <ul> <li>Graph Type - Change the graph from a simple linear graph to using D3 modes such as step after (it will appear like a step graph) or natural (a more curved graph)</li> <li>Line Opacity - Sets the opacity of the line that is drawn.  This can be useful if you want to remove the solid color of the line and just use the area under the graph.</li> <li>Max Graph - All values that are non-zero will automatically scale to the maximum value of the graph.  This is handy if you want to draw attention to areas of a track that are important.</li> <li>Graph Area - If this is set to true it will shade in the area under the graph based on the color and opacity that is chosen.</li> </ul>"},{"location":"UI-AttributeTimeline/#attention-regions","title":"Attention Regions","text":"<p>In the image above one of the graphed attributes will go from 1 to 0 at specific frame times.  Using the graph settings it can appear to highlight regions by: * setting the graph type to 'step after' * setting the line opacity to 0 * setting it to use the Max Graph setting * setting it to graph the area under the graph</p> <p>This is a simple way to get highlighted regions of a track to draw attention to the user.</p>"},{"location":"UI-AttributeTimeline/#y-axis-adjustments","title":"Y-Axis Adjustments","text":"<p>Besides setting the Y-Axis Range in the settings for the graph, the Y-Axis range can be adjusted at any time by hovering over the Y-Axis and double clicking.</p> <p></p> <p></p> <p>The Axis will update as you change the values and you can click Save to accept the new Axis range.  NOTE:  This will not change TimelineGraph Settings Y-Axis if they are set so reloading will not persist the new range.  It is meant to adjust the range on the fly for viewing data.</p>"},{"location":"UI-Attributes/","title":"Attributes","text":""},{"location":"UI-Attributes/#concepts-and-terms","title":"Concepts and Terms","text":"<ul> <li>Attribute Definitions are templates.  They have a name and a value type, such as <code>String</code>, <code>Number</code>, or <code>Boolean</code>.  Definitions must be created before attribute values can be assigned.  Tracks and detections each have their own set of definitions.</li> <li>Track Attributes apply to an entire track. Each track can only have one value for each track attribute definition.</li> <li>Detection Attributes can be different for every frame in a track.</li> </ul>"},{"location":"UI-Attributes/#example-attribute-definition","title":"Example Attribute Definition","text":"<ul> <li>Track Attributes:<ul> <li>CompleteTrack: <code>Boolean</code></li> <li>FishLength: <code>number (cm)</code></li> </ul> </li> <li>Detection Attributes:<ul> <li>Swimming: <code>Boolean</code></li> <li>Eating: <code>Boolean</code></li> </ul> </li> </ul>"},{"location":"UI-Attributes/#example-attribute-values","title":"Example Attribute Values","text":"<ul> <li>Fish Track 1<ul> <li>Track Attributes<ul> <li><code>{ \"FishLength\": 20 }</code></li> </ul> </li> <li>Detection Attributes<ul> <li>Frame 1<ul> <li><code>{ \"Eating\": True }</code></li> </ul> </li> <li>Frame 2<ul> <li><code>{ \"Swimming\": False, \"Eating\": True }</code></li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Info</p> <p>All Attribute definitions do not need to be assigned to values.  CompleteTrack (Track Attribute) and Swimming for Frame 1 (Detection Attribute) weren't assigned in this example.</p>"},{"location":"UI-Attributes/#using-the-attributes-panel","title":"Using the Attributes Panel","text":"<ol> <li>Select an existing track or detection with left click.</li> <li>Open the Track Details page by clicking on the  button in the Type List area or pressing the A key.</li> <li>Here you will see the track/detection type, confidence pairs associated with it and then a list of track and detection attributes.</li> <li>For attributes there are two sections<ol> <li>Track Attributes - All track level attributes</li> <li>Detection Attributes - attributes associated with the track on a per frame basis</li> </ol> </li> <li>Attributes can be sorted by their name (alphabetically) or by their numeric value.  Clicking on the  or the  button will swap between the two.</li> <li>Attribute Filtering<ol> <li>The Attribute filtering icon  will change color when filtering is being applied.</li> <li>Clicking on the filter icon will bring up the Attribute Details Panel where Attributes Filtering and Attributes Timeline Graphing can be done.</li> </ol> </li> </ol> <p>Info</p> <p>Attributes found during import in a VIAME CSV will automatically show up in the list.  The data type of the attribute is guessed by examining values and may need to be manually corrected.</p> <p>By default, all attributes associated with the dataset are visible and editable.  You can hide unused attributes by clicking the  toggle next to  Attribute.</p> Show Unused  Hide Unused"},{"location":"UI-Attributes/#creating-attribute-definitions","title":"Creating Attribute Definitions","text":"<ol> <li>Click on the  Attribute icon for in either the track or detection attribute area.<ol> <li></li> </ol> </li> <li>Enter a unique name</li> <li>Choose a Datatype<ol> <li><code>Number</code></li> <li><code>Boolean</code> (True/False)</li> <li><code>Text</code><ol> <li>Custom text that the user provides</li> <li>A predefined list of text to choose from, separated by newline.</li> </ol> </li> </ol> </li> <li>Click Save to add the new attribute</li> </ol>"},{"location":"UI-Attributes/#editing-attribute-definitions","title":"Editing Attribute Definitions","text":"<p>Click the  button next to an existing attribute to edit its definition.</p> <p></p> <p>Warning</p> <p>Editing or deleting an attribute definition doesn\u2019t change any existing attribute values.</p> <ul> <li>Deleting an attribute definition will cause it to disappear from the list, but the values will remain in the database.</li> <li>Editing an attribute definition will change the way the controls behave, but will not change any existing set values.</li> </ul> <ul> <li>User Attribute - This flag will set the attribute so that the storage of data is per user instead of globally.  By defauly attributes are stored on the dataset and are universal for each user that views/edits the dataset.  If this flag is set the attributes will be per user so that different user's when setting attributes will see different values.  This is stored in the TrackJSON structure under 'UserAttributes' key for track and detection attributes.  There is a new Sidebar called User Attribute Review which allows for reviewing of all user attributes.</li> <li>Color - Allows specification of a custom color to represent the attribute when filtering or when graphing the attribute value</li> </ul>"},{"location":"UI-Attributes/#attribute-shortcuts","title":"Attribute Shortcuts","text":"<p>A specific key shortcut can be assigned to setting the value of an attribute.  When the user presses this key combination the Attribute can be set, unset, or prompt the user for a value.</p> <ul> <li>Edit Keys - After clicking this button put in a keyboard combination which you want to use to assign a value to the attribute.  There are reserved shortcuts and the dialog will inform you if you're using a reserved shortcut.</li> <li>Type <ul> <li>set - set the value to a specific defined numerical/text/boolean value</li> <li>remove - removes the value from the attribute and resets it back to empty</li> <li>dialog - a dialog pops open asking the user for input for the attribute value</li> </ul> </li> <li>Description - a text based description of the shortuct.  This description is used in the Help dialog to show what all the keyboard shortcuts are.</li> </ul> <p></p> <p>In the upper right of the screen the keyboard icon is used to toggle on/off system and attribute shortcuts. The info icon next to it will display a list of possible shortcuts that are set and will use the Description to explain what a shortcut does.</p>"},{"location":"UI-Attributes/#attribute-value-colors","title":"Attribute Value Colors","text":"<p>Attributes that are of type text can have their colors preset and saved in the configuration file.  If your Attribute type is of type Text you have an additional tab that allows you to set the color for each state that is calculated to be in the system.</p> <p>These colors can be used in the Attribute Rendering or the Swimlane views for attributes to properly render the system.</p> <p></p> <p>Added the capability to create color gradients for Attribute Values.  This will allow numerical values to have custom color gradients which can be used in swimlanes, or in displaying the values of attributes as well.</p> <p>This works by configuring numerical values and assigning them a color.  The color gradient will be automatically generated from the values.  If the options for color is left as auto in Attribute Rendering, or if a Swimlane is chosen it will utilize these color gradient scales.</p>"},{"location":"UI-Attributes/#setting-attribute-values","title":"Setting Attribute Values","text":"<ol> <li>Click on the attribute value when in viewing mode to edit and set the attribute</li> <li>Or directly edit the value field when in the attribute editing mode</li> <li>Setting an attribute to the empty value will remove the value from the track/detection</li> </ol>"},{"location":"UI-Attributes/#importing-and-exporting-attributes","title":"Importing and Exporting Attributes","text":"<p>Attributes are part of the dataset configuration that can be imported and exported.</p> <ol> <li>Set up a dataset with all the attributes you need</li> <li>In the  Download menu, choose Configuration.</li> <li>Use this configuration with other datasets<ol> <li>Use the  Import button to load this configuration to other datasets.</li> <li>Upload the configuration file when you create new datasets to initialize them with these attribute definitions.</li> </ol> </li> </ol>"},{"location":"UI-Attributes/#applying-attributes-demo","title":"Applying Attributes Demo","text":""},{"location":"UI-Configuration/","title":"Configuration","text":"<p>DIVE-DSA has the capability to specify configurations for datasets.  Configurations can customize the user interface and limit the capability of standard users to access features within DIVE.  Things like preventing Editing, Hiding uneeded features can be configured. </p> <p>Additionally launch actions can be configured to select specific tracks or go to specific timeframes based on conditions specified.</p> <p>Along with launch actions, special keyboard shortcuts can be configured to perform actions, like selecting tracks or going to specific frames in tracks.</p>"},{"location":"UI-Configuration/#general","title":"General","text":"<p>The general setting is where you configure the location of the configuration JSON data.  The Configuration can be at any level in the folder hierarchy.  If it is higher in the level all sub folders will use the same configuration.  This is a way that the configuration can be unified amongst all dataset in a folder.</p> <p>In the dropdown you can choose the folder where the configuration lives.  If you are changing it to a sub folder or a parent folder you can use the 'Transfer' button to move the configuration to the higher level.</p>"},{"location":"UI-Configuration/#ui-settings","title":"UI Settings","text":"<p>UI settings enable toggling of differnt UI elements in the UI for all users. For more details I would go specifically to the UI-Settingssection to view the elements</p>"},{"location":"UI-Configuration/#launch-actions","title":"Launch Actions","text":"<p>Launch Actions are actions that are created on launch which automatically trigger certain behaviors. There are two types of launch actions currently. For more details I would go specifically to the UI-Actionssection to view the details.</p>"},{"location":"UI-Configuration/#action-shortcuts","title":"Action Shortcuts","text":"<p>Action shortcuts are similar to launch actions except they are triggered by a customizable keyboard shortcut instead of being triggered on launch.  Any Action configured in Launch Actions can be configured to a shortcut.  This also includes the capability to select next/previous for tracks and timeframes that match conditions. For more details I would go specifically to the UI-Actionssection to view the details.</p>"},{"location":"UI-Configuration/#ui-configuration-json","title":"UI Configuration JSON","text":"<p>The User interface configuration is stored in the metadata of the Dataset Folder, or the parent folder which contains the item.</p> <p>Go to UI-ConfigurationJSON for more information</p>"},{"location":"UI-ConfigurationJSON/","title":"UI-Configuration JSON","text":"<p>Below is a base UI Configuration JSON will all items turn on. If you see any of these values to false it will turn off the top level item.  If the top level item is turned into an Object it will then read the settings of that object.  You can note that below in the other documentation.</p> <pre><code> \"UISettings\": {\n        \"UIContextBar\": true,\n        \"UIControls\": true,\n        \"UIInteractions\": true,\n        \"UISideBar\": true,\n        \"UITimeline\": true,\n        \"UIToolBar\": true,\n        \"UITopBar\": true,\n        \"UITrackDetails\": true\n    },\n</code></pre> <p>Below is the configuration JSON for a view with all of the items turned off</p> <pre><code>\"configuration\": {\n    \"UISettings\": {\n        \"UITopBar\": {\n            \"UIData\": false,\n            \"UIJobs\": false,\n            \"UINextPrev\": false,\n            \"UIToolBox\": false,\n            \"UISlicerCLI\": false,\n            \"UIImport\": false,\n            \"UIExport\": false,\n            \"UIClone\": false,\n            \"UIConfiguration\": false,\n            \"UIKeyboardShortcuts\": false,\n            \"UISave\": false\n        },\n        \"UIToolBar\": {\n            \"UIEditingInfo\": false,\n            \"UIEditingTypes\": [\n                false,\n                false,\n                false\n            ],\n            \"UIVisibility\": [\n                false,\n                false,\n                false,\n                false,\n                false\n            ],\n            \"UITrackTrails\": false\n        },\n        \"UISideBar\": {\n            \"UITrackTypes\": false,\n            \"UIConfidenceThreshold\": false,\n            \"UITrackList\": false,\n            \"UIAttributeSettings\": false,\n            \"UIAttributeAdding\": false,\n            \"UIAttributeUserReview\": false\n        },\n        \"UIContextBar\": {\n            \"UIThresholdControls\": false,\n            \"UIImageEnhancements\": false,\n            \"UIGroupManager\": false,\n            \"UIAttributeDetails\": false,\n            \"UIRevisionHistory\": false,\n            \"UIDatasetInfo\": false\n        },\n        \"UITrackDetails\": {\n            \"UITrackBrowser\": false,\n            \"UITrackMerge\": false,\n            \"UIConfidencePairs\": false,\n            \"UITrackAttributes\": false,\n            \"UIDetectionAttributes\": false\n        },\n        \"UIControls\": {\n            \"UIPlaybackControls\": false,\n            \"UIAudioControls\": false,\n            \"UISpeedControls\": false,\n            \"UITimeDisplay\": false,\n            \"UIFrameDisplay\": false,\n            \"UIImageNameDisplay\": false,\n            \"UILockCamera\": false\n        },\n        \"UITimeline\": {\n            \"UIDetections\": false,\n            \"UIEvents\": false\n        },\n        \"UIInteractions\": {\n            \"UISelection\": false,\n            \"UIEditing\": false\n        }\n    }\n}\n</code></pre>"},{"location":"UI-DatasetInfo/","title":"Dataset Info","text":"<p>Information about the dataset can be displayed within the DIVE-DSA annotation tool be applying metadata to the folder of the dataset.</p> <p></p> <p>Utilizing the girder interface to add metadata or using the endpoint  <code>PUT /folder/{girder_id}/metadata</code>  with the key of <code>datasetInfo</code> will allow for meatadat to be added to the folder which will then be displayed in the user interface.</p>"},{"location":"UI-Group-Manager/","title":"Group Manager","text":"<p>The group manager is one pane of the context sidebar.</p>"},{"location":"UI-Group-Manager/#feature-overview","title":"Feature overview","text":"<p>DIVE supports complex group annotation.</p> <ul> <li>Groups can be interpreted as activity participation.  For example, two <code>Person</code> type tracks participate in a <code>Conversation</code> activity.</li> <li>Groups can be used to represent most types of multi-annotation collections.</li> <li>Group membership for tracks can be explicitly constrained to sub-intervals within <code>[track.begin, track.end]</code>.  A single track can belong to a group for multiple sub-ranges, typically interpreted as the tracked object leaving and re-joining a group.</li> <li>Tracks can belong to many different groups at once, and can have different participation interval(s) for each.</li> </ul> <p>See the data format documentation for the complete capabilities of group annotations in the DIVE json schema.</p>"},{"location":"UI-Group-Manager/#group-list-controls","title":"Group List Controls","text":""},{"location":"UI-Group-Manager/#group-type-list","title":"Group Type List","text":"<p>The group type summary list allows for enabling and disabling annotations based on their group and bulk-editing group type characteristics.</p> <ul> <li>Groups do not have separate \"selected\" and \"editing\" states, so selecting a group puts it into editing mode.</li> <li>Group type styles can be edited in the same fashion as track type styles in the left sidebar using  (the edit pencil).</li> </ul>"},{"location":"UI-Group-Manager/#group-instance-list","title":"Group Instance List","text":"<p>Each group instance includes the following.</p> <ul> <li>A checkbox to enable and disable visibility</li> <li>The group id</li> <li>The group type</li> <li>A list of member track ids that are part of the group.</li> </ul>"},{"location":"UI-Group-Manager/#group-editor","title":"Group Editor","text":"<p>To enter group edit mode, click a group's ID number in the group list.</p> <ul> <li>Add new tracks to a group by first entering group edit mode, then selecting tracks to add (in the annotation window, the sidebar, or any UI where track selection can happen)</li> <li> (next to a track) will remove a track from a group.</li> <li>Use the frame range input boxes to adjust the start and end frame numbers that a track participates in a group.</li> <li> will set a frame input box to the current frame.</li> <li> will create a new sub-interval participation range for a track within a group.</li> <li> will remove a sub-interval participation range.</li> <li> Delete Group will delete a group without deleting its member tracks.</li> </ul> <p>Some notes about group editing behavior.</p> <ul> <li>If you delete a track, and the track was the only track remaining in one or more groups, those groups will also be deleted.</li> <li>If you delete a group, its member tracks will not be deleted no matter how many members there are.</li> <li>The group's composite range (shown as disabled begin and end frames in the group editor) is the maximum overlapping range of all sub-intervals of all member tracks.</li> </ul>"},{"location":"UI-Group-Manager/#example-data","title":"Example data","text":"<p>The group feature was initially developed for compatibility with the The Multiview Extended Video with Activities (MEVA) dataset.  Find example data at mevadata.org.</p>"},{"location":"UI-Navigation-Editing-Bar/","title":"Navigation and Editing Bar","text":""},{"location":"UI-Navigation-Editing-Bar/#navigation-bar","title":"Navigation Bar","text":"<p>The navigation bar is the row of controls at the very top of the window.</p> <ul> <li> Data navigates to the folder that contains the current dataset.</li> <li> Import allows the upload of several kinds of files<ul> <li>overwrite the current annotations with a <code>.json</code> or <code>.csv</code> annotation file.</li> <li>overwrite the style and attribute configuration with a config <code>.json</code> file.</li> </ul> </li> <li> Download (Web) or  Export (Desktop) allows for exporting all or part of the current dataset.<ul> <li>Exclude Tracks - this allows you to remove tracks below a specific confidence threshold when exporting the CSV.  It is how you can export only the higher detections/tracks after running a pipeline.</li> <li>Checked Types Only - allows you to only export the annotations of types that are currently checked in the type list.</li> <li>Web-specific options are documented in the web download section</li> </ul> </li> <li> Clone is documented in the web clone section.</li> <li> Help provides mouse/keyboard shortcuts as well as a link to this documentation.</li> <li> is used to save outstanding annotation changes and any custom styles applied to the different types.  Changes are not immediately committed and will instead update the save icon with a number badge indicating how many changes are outstanding.  Clicking this button will commit your changes and reset the count to zero.</li> </ul>"},{"location":"UI-Navigation-Editing-Bar/#editing-bar","title":"Editing Bar","text":"<p>The editing bar is the second row below navigation.</p>"},{"location":"UI-Navigation-Editing-Bar/#editing-status-indicator","title":"Editing Status Indicator","text":"<p>On the far left, the editing mode status indicator shows you what mode you're in, what input is expected, and usually reminds you to press Esc to cancel.</p>"},{"location":"UI-Navigation-Editing-Bar/#edit-mode-toggles","title":"Edit Mode Toggles","text":"<p>Editing mode toggles control the type of geometry being created or edited during annotation.  See the Annotation Quickstart for an in-depth guide to annotation.</p>"},{"location":"UI-Navigation-Editing-Bar/#visibility-toggles","title":"Visibility Toggles","text":"<p>The  visibility section contains toggle buttons that control the different types of annotation data can be hidden or shown.</p> <ul> <li> toggles rectangle visibility</li> <li> toggles polygon visibility</li> <li> toggles head/tail line visibility</li> <li> toggles annotation type &amp; confidence text visibility</li> <li> toggles a cursor hover tooltip, helpful for reviewing very dense scenes with lots of overlap.</li> <li> toggles track trail visibility.  The track trail is configurable to show up to 100 frames both ahead and behind each bounding box.  The trail line is made of bounding box midpoints.</li> </ul>"},{"location":"UI-Notifications/","title":"UI Notifications","text":"<p>UI Notifications provide a way for external events/tasks to provide notifications to users that currently have the system open.  This is especially useful if there are short running Slicer-CLI-Web tasks that want to provide information to the user.  I.E. a quick Slicer-CLI task that finds a specific track and frame combination and wants the user to navigate to the specific location.</p> <p>When a notification is detected on the client a dialog window will be displayed with custom text specified from the JSON body as well as indication of what actions will be performed such as \"selecting TrackId 3\" and \"Going to Frame 200\".</p>"},{"location":"UI-Notifications/#rest-endpoint","title":"REST Endpoint","text":"<p>There is a REST endpoint: <code>POST /dive_rpc/ui_notificaitions/{id}</code> that takes in a DatasetId and JSON body response to display the notification to the same user with the same DatasetId currently open.  I.E. If you run a task the task is able to provide information back to the user in the form of a Dialog box and provide suggestion actions like selecting a track or going to a specific frame.</p>"},{"location":"UI-Notifications/#json-body-data","title":"JSON Body Data","text":"<p>The JSON format is relatively simple but may be updated in the future to have additional fields and provide further actions.</p> <ul> <li>text (string) - This is a required string that is displayed to the user.  It provides some context to the user about what is being selected or why data is being changed</li> <li>reloadAnnotations (boolean)  - If this is true it is the only action that will be performed.  Instead of performing any other actions it will reload the page. <li>selectedTrack (number)  - integer number to select a specific track <li>selectedFrame (number)  - interger frame number to seek to that frame."},{"location":"UI-Notifications/#sample-json-body-data","title":"Sample JSON Body Data","text":"<pre><code>{\n    \"text\": \"Sample Notification to provide to user\",\n    \"selectedFrame\": 300,\n    \"selectedTrack\": 2,\n}\n</code></pre>"},{"location":"UI-Notifications/#adding-actions-to-ui-notifications","title":"Adding Actions to UI Notifications","text":"<p>The below example utilizes the UI-Actions to select a track that meets seom requirements. You can specify a list of <code>diveActions</code> that can be executed sequentially The below action will select the next track that is of type <code>mouse</code> and has a detection attribute named <code>mouseState</code> that is <code>=</code> to <code>scratching</code> the UI-Actions pages has more information about the existing actions and their capabilities and configuration.</p> <pre><code>{\n    \"text\": \"Testing the Actions Listing\",\n    \"diveActions\": [\n        {\n            \"description\": \"This is a Description of the action\",\n            \"actions\": [\n                {\n                    \"action\": {\n                        \"track\": {\n                            \"Nth\": 0,\n                            \"attributes\": {\n                                \"detection\": {\n                                    \"mouseState\": {\n                                        \"op\": \"=\",\n                                        \"val\": \"scratching\"\n                                    }\n                                }\n                            },\n                            \"direction\": \"next\",\n                            \"startFrame\": -1,\n                            \"startTrack\": -1,\n                            \"type\": \"TrackSelection\",\n                            \"typeFilter\": [\n                                \"mouse\"\n                            ]\n                        },\n                        \"type\": \"GoToFrame\"\n                    }\n                }\n            ]\n        }\n    ]\n}\n</code></pre> <p>Below adds a keyboard shortcut that will be added tied to the action.  Now when hitting <code>shift+p</code> it will perform the actions.</p> <pre><code>{\n    \"text\": \"Testing the Actions Listing\",\n    \"diveActions\": [\n        {\n            \"description\": \"This is a Description of the action\",\n            \"shortcut\": {\n                \"key\": \"p\",\n                \"modifiers\": [\"shift\"]\n            },\n            \"actions\": [\n                {\n                    \"action\": {\n                        \"track\": {\n                            \"Nth\": 0,\n                            \"attributes\": {\n                                \"detection\": {\n                                    \"mouseState\": {\n                                        \"op\": \"=\",\n                                        \"val\": \"scratching\"\n                                    }\n                                }\n                            },\n                            \"direction\": \"next\",\n                            \"startFrame\": -1,\n                            \"startTrack\": -1,\n                            \"type\": \"TrackSelection\",\n                            \"typeFilter\": [\n                                \"mouse\"\n                            ]\n                        },\n                        \"type\": \"GoToFrame\"\n                    }\n                }\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"UI-Notifications/#type-definitions","title":"Type Definitions","text":"<p>UI Notification Type definition. Note:  a <code>?</code> at the end of a key means it is optional in Typescript <pre><code>export interface UINotification {\n    datasetId: string[];\n    text: string;\n    selectedFrame?: number;\n    selectedTrack?: number;\n    reloadAnnotations?: boolean;\n    diveActions?: UIDIVEAction[];\n}\n</code></pre></p> <p>The UIDIVEAction allows for executing actions or for creating shortcuts. A More detailed listing of this is available at: UI-Actions</p> <pre><code>export interface UIDIVEAction {\n  shortcut?: {\n    key: string;\n    modifiers?: string[]; //list of modifiers for the keyboard shortcut\n  }\n  description?: string;\n  applyConfig?: boolean; //Add to the system configuration the shortcut/action\n  actions: DIVEAction[];\n}\n</code></pre>"},{"location":"UI-Settings/","title":"UI Settings","text":"<p>The main section of the UI-Configuration where different elements of the UI can bet turned on and off</p>"},{"location":"UI-Settings/#main","title":"Main","text":"<p>Under the Main Setting as you disable this main groups the submenus will be removed.  I.E If you remove the 'Track Details' you no longer can customize what is visible in the Track Details Subsection.</p> <ul> <li>Interactions - Disable User interactions for editing/creating tracks.</li> <li>Top Bar - Removes the entire Top Bar (For Owners/Admins the Configuration button will remain)</li> <li>Tool Bar - Removes the Tool Bar containing the Editing/Viewing Options</li> <li>SideBar - Removes the Side Bar containing the Track List and Track Types.  If you remove this by default the Track Details will be enabled.</li> <li>Context Bar - Removes all context bar (Revision History, Attribute Filtering/Graphing, Groups, Threshold Controls )</li> <li>Track Details - Removes the Track Details pages (Left side of the screen, the alternate view of the Type List/ Track List)/</li> <li>Media Controls - The playback controls for the video/image sequence</li> <li>Timeline - The bottom timeline visibility.</li> </ul>"},{"location":"UI-Settings/#interactions","title":"Interactions","text":"<ul> <li>UI Selecting/Deselecting - Disable the user from selecting/deselecting tracks.  This is useful if you are using Launch Actions or Action Shortcuts in the configuration to automatically select tracks.</li> <li>UI Editing Annotations - This will prevent the user from being able to enter edit mode and change the annotation.  They still have the capability to set/change Attributes, they just can't change/edit tracks using the standard methods or keyboard shortcuts.</li> </ul>"},{"location":"UI-Settings/#topbar","title":"TopBar","text":"<p>Most of the options are pretty clear.  It turns off and on different buttons in the UI.  NOTE:  If you are the owner of the dataset or an Admin the configuration button will always appear even if turned off.  This is to ensure that you can change the configuration through the UI if you hide the option from standard users.</p>"},{"location":"UI-Settings/#toolbar","title":"ToolBar","text":"<p>The ToolBar is used for editing tracks and changing the visualization of tracks.  The Editing Types and Visible Types are multi-select which allows for customization of the types of annotations that can be edited/created and the visbility of specific types of annotations.  It may not be necessary for the annotator to know about line or polygon types so you can remove them from the menu if not needed.</p>"},{"location":"UI-Settings/#sidebar","title":"SideBar","text":"<ul> <li>Track Types - hide the Track type listing on the left side of the screen</li> <li>Confidence Threshold - Disable/Hide the confidence threshold to prevent annotators from changing the value</li> <li>Track List - Hide the entire track list</li> <li>Track Details - Hide the Track details (The alternative view of the left sidebar)</li> <li>Attribute Settings - Hide the Attribute settings to prevent users from adding/modifying attributes</li> <li>Adding Attributes - Prevent users from adding additional attributes to the Dataset</li> <li>Attribute User Review - When you have User attributes Attribute User review allows for a user to see all of the other user's attributes and review them.</li> </ul>"},{"location":"UI-Settings/#contextbar","title":"ContextBar","text":"<p>Hide or enable the additional contextual menus on the right side of the screen.</p>"},{"location":"UI-Settings/#track-details","title":"Track Details","text":"<p>Customize the interface of the track Details.  The details of Track/Detection Attributes are customized more in SideBar (Specifically enabling adding/editing and customization of Attributes)</p>"},{"location":"UI-Settings/#playback-controls","title":"Playback Controls","text":"<p>Customize the interface on the playback controls to hide interfaces not needed.</p>"},{"location":"UI-Settings/#timeline","title":"Timeline","text":""},{"location":"UI-Timeline/","title":"Timeline","text":"<p>The timeline provides a control bar and a few different temporal visualizations.  All timeline visualizations are updated live by type confidence slider(s), type checkboxes, and track checkboxes.</p> <p></p> <p></p>"},{"location":"UI-Timeline/#control-bar","title":"Control Bar","text":"<ul> <li> will minimize the timeline.</li> <li>Detections button selects the Detection Count histogram timeline view.</li> <li>Events button selects the Event View, which is a Gantt-style track chart.</li> <li>Groups button selects the Group View, which is a Gantt-style group chart.</li> <li>  are standard media playback controls.</li> <li>  opens the video playback speed controls and volume controls, respectively.</li> <li>frame ## shows the current frame number.</li> <li> will enable camera lock, which causes the annotation view to auto-zoom and pan to whatever annotation is currently selected.  This is useful when reviewing the output of a pipeline.</li> <li> or the R key will reset zoom/pan in the annotation view.</li> <li> will open the image contrast adjustment panel.</li> </ul>"},{"location":"UI-Timeline/#detection-count","title":"Detection Count","text":"<p>This is the default visualization.  It is a stacked histogram of track/detection types over the duration of the sequence.</p> <ul> <li>Line color matches the annotation type style.</li> <li>Top green line is the sum count of all annotations of all types on each frame.</li> </ul>"},{"location":"UI-Timeline/#event-view","title":"Event View","text":"<p>The event viewer shows the start/stop frames for all tracks.  It is a kind of Gantt chart, also similar to a swimlane chart but with more compact packing.</p> <ul> <li>The tracks are drawn using their corresponding type color.</li> <li>When hovering over any track the TrackID will display.</li> <li>Clicking on a track will select it and jump to the track at the selected frame.</li> </ul>"},{"location":"UI-Timeline/#group-view","title":"Group View","text":"<p>The group viewer is just like the event viewer, but shows the start and end times of track groups, colored by group types. Switching to the group view changes the coloring scheme of annotations in the annoation window.</p>"},{"location":"UI-Timeline/#interpreting","title":"Interpreting","text":"<p>Single frame detections are presented as single frames with spaces between.</p> <p> </p> <p>A selected track will be cyan and will cause all other tracks to fade out.  If a selected track is solid cyan, that means every frame in the track is a keyframe.</p> <p> </p> <p>A selected interpolated track will show the areas of interpolation as yellow lines, the keyframes as cyan ticks, and gaps as empty regions.</p>"},{"location":"UI-TimelineSettings/","title":"Timeline Settings","text":"<p>Customizing the timeline as well as adding filtered timelines can be done through the Configuration -&gt; Timeline Settings menu.</p> <p>The Custom Timeline List is used to layer multiple timelines on top of each other so they can be displayed simulatneously.  This will replace the default list of timelines that are available with a custom style.</p>"},{"location":"UI-TimelineSettings/#custom-timeline-list","title":"Custom Timeline List","text":"<p>In the dropdown list are available timelines that can be added to the main view.  These include the default Detection/Events timelines, any Attribute Swimlane Timelines, Attribute Graphing Timelines, or any Filter Timelines.</p> <p>The Max Timeline Area Height is used to adjust the height of the timline at the bottom of the screen to larger value to accomodate more data.</p> <p>After Adding a Timeline you can click on the Edit button to modify the details of how the timeline works</p>"},{"location":"UI-TimelineSettings/#custom-timeline-editing","title":"Custom Timeline Editing","text":"<p>The editing for invidual timelines will appear directly in the editing as a new card.</p> <ul> <li>Max Height - Sets the max height for this timeline.  Setting it to -1 will auto size the timeline based on the available space.</li> <li>Order - Starting with 0 at the top will set the order of the timelines.</li> <li>Dimissable - allows the user to hide the timeline if not desired.  They can renable it using the menu options afterwards.</li> </ul> <p></p> <p>The charts are then placed in the timeline area.  Each chart is dismissable so there is an X in the corner which can be clicked to remove the chart. If a chart is removed it then appears in the menu with an \"Enable\" button to add the chart back to the timeline area.</p> <p></p> <p>Clicking \"Enable\" will display any charts that have been dismissed.</p>"},{"location":"UI-TimelineSettings/#timeline-event-filters","title":"Timeline Event Filters","text":"<p>The Filter Timelines allows for the creation of the base Event Timeline but with a specific filter to only tracks that meet specific conditions. THe Filtering used to display the tracks in the chart are similar to the filters used in the Actions Editor</p> <ul> <li>Name - A Name to display in the configuration, or as a title when selecting timelines</li> <li>Enabled - Enables the timeline so it is visible</li> <li>Confidence Filter - Sets a filter so only tracks with a confidence value above it are displayed.  -1 is a default to ignore the confidence filter.</li> <li>Filter Types - Provide a list of types of tracks that should be displayed in this filtered view.</li> <li>Track Attribute - Set a condition where the tracks needs to have X track attribute value to be displayed in this list.</li> <li>Detection Attribute - Sets a filter so only tracks which have a specific detection attribute are displayed in the list.</li> </ul>"},{"location":"UI-Track-List/","title":"Track List","text":""},{"location":"UI-Track-List/#track-list-controls","title":"Track List Controls","text":"<p>The track list allows for selecting and editing tracks.  A selected track will look different depending on whether it's a single detection or a multi-frame track.</p> <ul> <li> opens track creation settings</li> <li> deletes all tracks in the track list</li> <li> Track/Detection begins creation of a new annotation.</li> </ul>"},{"location":"UI-Track-List/#single-detection","title":"Single Detection","text":"<p>A track that spans a single frame.</p> <ul> <li> deletes the entire detection annotation</li> <li> goes to the first frame of the detection</li> <li> selects the detection and toggles edit mode.</li> </ul> <p>Warning</p> <p>The  button will remove the whole track if it's longer than a single detection.  To remove individual keyframes, use  (the keyframe toggle button).</p>"},{"location":"UI-Track-List/#multi-frame-track","title":"Multi-frame track","text":"<p>A track that spans multiple frames and has more options</p> <ul> <li> deletes the entire track</li> <li> splits the track into 2 smaller tracks on the current frame.</li> <li> is filled in if the current frame annotation is a keyfame.  Clicking this will either remove the keyframe if it exists or make the current interpolated annotation a keyframe.</li> <li> turns interpolation on/off for the interval between keyframes.</li> <li> jumps to the first frame of track</li> <li> jumps to the previous keyframe</li> <li> jumps to the next keyframe</li> <li> jumps to the last frame of the track</li> <li> selects the detection and toggles edit mode.</li> </ul>"},{"location":"UI-Type-List/","title":"Type List","text":""},{"location":"UI-Type-List/#type-list-controls","title":"Type List Controls","text":"<p>Each dataset maintains its own list of types, and types can be defined on-the-fly.</p> <p>The Type List is used to control visual styles of the different types as well as filter out types that don't need to be displayed.</p> <ul> <li>The checkbox next to each type name can be used to toggle types on and off.</li> <li> toggles the sort order between alphabetical and by number of annotations of each type.</li> <li> opens the type settings menu.</li> <li> will remove the type from any visible track or delete the track if it is the only type.</li> <li> will switch the left sidebar panel to show the track attribute editor (and group editor) view.</li> </ul>"},{"location":"UI-Type-List/#type-style-editor","title":"Type Style Editor","text":"<p>The type style editor controls the visual appearance of annotations in all other areas of the application.  Launch the editor by hovering over a type row in the list and clicking  (the edit pencil).</p> <ul> <li>Type Name - You can change the name for the type and it will update all subsequent tracks that are using that Type.</li> <li>Show Label - show the type name label in the text above each box.</li> <li>Show Confidence - show the confidence value in the text above each box.</li> <li>Box Border Thickness - the line thickness can be changed to make a type stand out more or less</li> <li>Fill - Fill allows the bounding box to be filled.  This is useful for annotation of background items in an image.</li> <li>Border &amp; Fill Opacity - The opacity of the lines and the fill can be set here.</li> <li>Color - The color for the type within the annotations and the timeline views.</li> </ul>"},{"location":"UI-Type-List/#type-settings-menu","title":"Type Settings Menu","text":"<p>Click the  button in the type list heading to open type settings.</p>"},{"location":"UI-Type-List/#ad-hoc-mode","title":"Ad-hoc mode","text":"<p>In ad-hoc mode, new object classes are added as you annotate.  The type list updates automatically when new classes are added or the last member of a class is deleted.</p> <ul> <li>Set Lock Types to off for ad-hoc type creation.</li> <li>Set Show Empty to still show manually defined types with no track/detection examples in the type list.</li> </ul>"},{"location":"UI-Type-List/#locked-mode","title":"Locked mode","text":"<p>In locked mode, only a specified list of classes can be used, and must be selected or autocompleted from the list for each object.</p> <ul> <li>Set Lock Types to on to constrain annotation types to those already defined.</li> <li>You can add new types using the  Types button under type settings.</li> </ul>"},{"location":"VideoConversion/","title":"DIVE Dataset and Video Conversion","text":""},{"location":"VideoConversion/#video-transcoding","title":"Video Transcoding","text":"<p>Not all videos are capable of being used on the web.  There are some requirements that need to be met for the video to be displayed in a web interface.  Typically those include having a web compatible container and codec.</p> <p>The Container is the format of the video, like .mp4, .avi, .mov and other. The Codec is the encoding of the data and codec include h264, h265, av1 and others.</p> <p>For simplicity  DIVE requires .mp4 containers in the h264 codec format.</p> <p>When a video is uploaded/processed it uses ffprobe to check the current container and codec formats.  If the codec and container don't match .mp4 and h264 it will start a Girder Job to conver the video using ffmpeg to the proper format.  Depending on the worker node this will typically take 10-30% of the realtime playback length of the video.  This varies based on resolution and other quality settings of the video.</p> <p>DIVE does transcoding by default.  This is done because transcoding can fix issues in videos if they weren't encoding properly to begin with.  Videos coming from devices that encode in real time like cameras sometimes have errors and issues during the encoding process.  The transcoding can fix some of these errors.  When uploading in the DIVE interface there is an option to 'Skip Transcoding'.  When this is selected the worker job will check and see if the video meets the following conditions:</p> <ul> <li>The video container is .mp4</li> <li>The video codec is h264</li> <li>There are no duplicate frames or other oddities in the first 5 seconds of the video</li> </ul> <p>If those conditions are met the video will not be transcoded.</p>"},{"location":"VideoConversion/#video-frame-rate","title":"Video Frame Rate","text":"<p>DIVE will only support videos which maintain a consistent framerate throughout the length of the video. DIVE relies on this information to properly align the visualization of annotations with specific timepoints in the video on a frame by frame basis.</p> <p>Annotation is better performed when the frame rate is a consistent multiple.  While DIVE supports frame rates of 29.97fps and 59.94fps there needs to be care in subsampling these frame rates if annotations are being done at a lower rate than the video.</p>"},{"location":"VideoConversion/#annotation-frame-rate","title":"Annotation Frame Rate","text":"<p>The Annotation Frame rate can be set to subsample the frame rate of the video.  During upload you can choose and FPS or default to the Video FPS.  The subsample rate is best to be a interger divisor of the frame rate.  I.E using a annotation framerate of 10 when the video is 30fps works best and using a annotation frame rate of 10 when the video is 24fps is not recommended.  </p>"},{"location":"VideoConversion/#post-process-conversion","title":"Post Process Conversion","text":"<ol> <li>When a video is uploaded it will create a folder which will place the default video inside of the folder</li> <li>The folder metadata will set <code>fps</code> to chosen Annotation FPS or -1 if the Annotation FPS is set to the video fps</li> <li>Additionally, the <code>type</code> metadata will be set to 'video' or 'image-sequence' depending on the uploaded media type.</li> <li>First it will look for any sibling CSV or JSON files and determine if they are annotation or configuration files and import them properly.  This will import VIAME CSV and TrackJSON or the configuration JSON files specified in (DataFormats)[DataFormats.md]</li> <li>The endpoint <code>dive_rpc/postprocess/{folderId}</code> is then used.  This endpoint will take the folder and look for images/videos in it and run ffprobe on videos to check the container format and the codec.<ol> <li>if the codec and container don't match h264 and mp4 it will transcode the video</li> <li>transcoding will also be done if there are frame errors discovered in the first 5 seconds of the video</li> <li>During the ffprobe process the default data for the video is recorded and added to the folder metadata as ffprobe_info</li> <li>The orignal FPS is also recorded as well as the originalFPS string for accuracy.</li> </ol> </li> <li>Once the video is complete and every succeeds it will add the metadata <code>annotate: true</code> to the folder to indicate that this is now a DIVE Dataset that can be viewed in the dive interface.</li> </ol>"},{"location":"VideoOverlay/","title":"Video Overlays","text":"<p>This includes some documentation for experimental features which may not be fully complete but are in place for testing purposes.</p>"},{"location":"VideoOverlay/#video-overlays_1","title":"Video Overlays","text":"<p>This allows for the configuration of a video that can be overlayed over the dataset base video.  It is useful for display heatmaps or other information on top of the current video.</p> <p>Requirements:</p> <ul> <li>The Overlay Video should be the same dimensions (width x height) of the original video, the same length (duration in seconds) and the same framerate.</li> <li>To implement video overlays a knowledge of how to add JSON metadata to Girder Folders and Items are required.</li> <li>There should be a folder in the dataset with the Metadata value of <code>overlayVideo: true</code>.  Make sure that the metadata is JSON.</li> <li>Within the above folder should be a video with Metadata value of <code>overlayVideoItem: true</code>. Make sure that the metadata is JSON.</li> <li>The system supports multiple video Overlays.  If you have multiple files it will display all of them and you can enable or disable each one.</li> </ul> <p>Having the above requirements met there will be a new icon in the visibile for the DIVE Dataset: </p> <p></p> <p>The Video Layer can be toggled on/off like any other annotation.</p> <ul> <li>name - A display name for the list of video overlays that are visible. </li> <li>Opacity - The global opacity of the video overlay</li> <li>Color Transparency - If there is metadata value set with transparency this will attempt to replace the color in the video and make it transparent.</li> <li>Override - Allows for specifying the current variance and color independently from the JSON file.  It's used to figure out the settings that you want to use for the system.  You can then click on the copy button to copy the JSON and past it into the metadata fro the video.</li> <li>ColorScale - Allows for rescaling grayscale heatmaps to a custom beginning and end color.  This is limited to two colors but will replace the black/white color range to whatever the user selects.</li> </ul>"},{"location":"VideoOverlay/#video-overlay-color-transparency","title":"Video Overlay Color Transparency","text":"<p>Utilizing another Metadata field on the video overlay item (<code>overlayVideoItem: true</code>) you can specify colors to be replaced in the video and made transparent. This is configured by adding a new Metadata tag to the video Item : <code>overlayMetadata: true</code></p> <p><pre><code>{\n    \"name\": \"Display Name for interface\"\n    \"transparency\": [\n        {\n            \"rgb\": [0, 255, 0],\n            \"variance\": 20,\n        }\n    ],\n    colorScale: {\n        black: '#000000',\n        white: '#FFFFFF'\n    }\n}\n</code></pre> Right now it only supports the first color in the array. Variance allows a +/- variance to the exact color that is selected. The colorScale isn't required but if it is found it will utilize it to scale grayscale imagery.</p>"},{"location":"VideoOverlay/#additional-video-settings","title":"Additional Video Settings","text":"<p>Utiliing similar code for the Video Overlay, the system can also allow the displaying of a video alongside the main video. This can be done by the same method above but using the a few additional JSON properties</p> <p><pre><code>{\n    positioning: {x: -1, y: 0, type: \"%\"},\n    scaling: {x: 0.5, y: 0.5}\n}\n</code></pre> Positioning takes in an X/Y coordinate or scaling factor.  'type' is optional and defaults to the value of 'px' which means it will use pixel offsets.  If you use the 'type' value of '%' it will use a percentage of the video width and height to position it.  'scaling' takes the original video and scales it up or down based on the numerical value. </p> <p>In the above example the video is scaled 50% in the x and y direction and then the positioning is offset so that the upper right hand corner of the video is one scaled width to the left of the current video.  Basically it scales the video and then moves it to the left of the current video display.</p>"},{"location":"Web-Version/","title":"Web Version","text":""},{"location":"Web-Version/#uploading-individual-files","title":"Uploading individual files","text":"<ul> <li>Open the DIVE Homepage, and navigate to the  Data tab.</li> <li>Click the  User Home button at the top left of the data browser.</li> <li>Click either your  Public or  Private folder, or make a new folder and navigate into it.</li> <li>Click the  Upload button that appears in the toolbar.</li> <li>Select a video or multi-select a group of image frames.<ul> <li>Use Ctrl or Shift to click every file you want to upload.</li> <li>If you already have <code>annotations.csv</code> or an annotation or configuration JSON select that too.</li> </ul> </li> <li>Choose a name for the dataset and enter the optional playback frame rate or select other optional files.</li> <li>Press Start Upload</li> <li>In the data browser, a new Launch Annotator button will appear next to your data<ul> <li>If you uploaded a video, it may need to transcode first</li> </ul> </li> </ul> <p>Info</p> <p>All video uploaded to the web server will be transcoded as <code>mp4/h264</code>.</p>"},{"location":"Web-Version/#uploading-zip-files","title":"Uploading zip files","text":"<p>A zip import can have one of the following file combinations:</p> <ul> <li>One or more images, an optional annotation file, and an optional configuration file</li> <li>One video with an optional annotation file and an optional configuration file</li> <li>One or more folders which contain the above examples (These will be converted to separate datasets)</li> </ul> <p>Zip import also accepts zip archive files that were generated by the Download Everything export button.</p>"},{"location":"Web-Version/#download-or-export-data","title":"Download or export data","text":"<p>Data can be downloaded from the FileBrowser by clicking the checkmark to the left of a dataset name.  This allows you to download the source images/video, the current detection file converted to <code>.csv</code> or everything including all backups of the detection files.</p> <ul> <li>Image Sequence or Video will export the source media as a <code>.zip</code></li> <li>Detections will export a VIAME <code>.csv</code> of annotations<ul> <li>Checkbox options are explained in the Navigation Bar Section.</li> </ul> </li> <li>Configuration will export a DIVE configuration <code>.json</code></li> <li>Everything will export all of the above.</li> </ul>"},{"location":"Web-Version/#sharing-data-with-teams","title":"Sharing data with teams","text":"<p>This information will be relevant to teams where several people need to work on the same data.</p>"},{"location":"Web-Version/#concepts","title":"Concepts","text":"<p>By default, data uploaded to your personal user space follows these conventions.</p> <ul> <li>Data in the  Public folder is readable by all registered users, but writable only by you by default.</li> <li>Data in the  Private folder is only visible to you by default.</li> </ul>"},{"location":"Web-Version/#working-with-teams","title":"Working with teams","text":"<p>A common scenario is for a group to have a lot of shared data that several members should be able to view and annotate.</p> <p>For most teams, we recommend keeping data consolidated under a single account then following the sharing instructions below to make sure all team members have appropriate access.</p> <p>It's easiest to create a single parent folder to share and then put all individual datasets inside that parent.</p> <p>Warning</p> <p>You should note that 2 people cannot work on the same video at the same time.  Your team should coordinate on who will work on each dataset.</p>"},{"location":"Web-Version/#managing-permissions","title":"Managing Permissions","text":"<p>DIVE uses Girder's Permissions Model.</p> <p>There are four levels of permission a User can have on a resource.</p> <ul> <li>No permission (cannot view, edit, or delete a resource)</li> <li>READ permission (can view and download resources)</li> <li>WRITE permission (includes READ permission, can edit the properties of a resource)</li> <li>ADMIN also known as own permission, (includes READ and WRITE permission, can delete the resource and also control access on it)</li> </ul>"},{"location":"Web-Version/#granting-access-to-others","title":"Granting access to others","text":"<ul> <li>Navigate to your data in the data browser.</li> <li> <p>Right click a dataset or a folder of datasets and choose Access Control</p> <p></p> </li> <li> <p>Search for and select users you want to grant access to.</p> </li> <li> <p>Select the correct permissions in the drop-down next to each user.</p> <p></p> </li> <li> <p>If this is a folder of datasets, enable the Include Subfolders switch.</p> </li> <li>Click Save.  These users should now be able to view and edit your data.</li> </ul>"},{"location":"Web-Version/#data-shared-with-you","title":"Data Shared with you","text":"<p>You can view data shared with you by selecting the  Shared With Me tab above the data browser.</p> <p></p>"},{"location":"Web-Version/#sharing-urls","title":"Sharing URLs","text":"<p>You can copy and paste any URL from the address bar and share with collaborators.  This includes folders in the data browser as well as direct links to the annotation editor.</p>"},{"location":"Web-Version/#dataset-clones","title":"Dataset Clones","text":"<p>A clone is a shallow copy of a dataset.</p> <ul> <li>It has its own annotations, and can be run through pipelines and shared with others.</li> <li>It references the media (images or video) of another dataset.</li> </ul> <p>Warning</p> <p>Be careful when deleting data that has been cloned.  Clones \"point to\" their source dataset for loading media, so if the source is deleted, all of its clones will fail to load.</p>"},{"location":"Web-Version/#clone-use-cases","title":"Clone use cases","text":"<ol> <li>When you want to use or modify data that doesn't belong to you, such as data from the shared training collection or from other users.</li> <li>When you want to run several different pipelines in parallel on the same input data and compare the results.</li> </ol> <p>Warning</p> <p>Merging cloned data back to the source is not currently supported.  To collaborate with others on annotations, the sharing use case above is preferred.</p>"},{"location":"Web-Version/#how-to-clone","title":"How to clone","text":"<ul> <li>Open the dataset you wish to clone by clicking Launch Annotator.</li> <li>Click the  Clone button in the top navigation bar on the right side.</li> <li>Choose a name and location for the clone within your own workspace.</li> </ul>"},{"location":"Web-Version/#revision-history","title":"Revision History","text":"<p>Revision history is accessible through the annotation UI in the Web version.  Each time you press  Save, a new revision of your annotation state is created.  It is possible to inspect (or \"check out\") past revisions.  The viewer will be in read-only mode when past revisions are checked out because only the most recent revision can be modified.</p> <ul> <li>Click  History in the Navigation Bar area to open the Revision History panel.<ul> <li>Each row shows the revision datetime, the action that caused it, and the number of additions and deletions.</li> <li>Click a row to check out a previous revision</li> </ul> </li> <li>Click  Download when a previous revision is checked out to download the annotation CSV from that revision.</li> <li>Click  Clone when a previous revision is checked out to create a new clone of the dataset from that revision.</li> </ul> <p>Info</p> <p>Revision roll-back is not yet supported, but will be added in a future update.  If you need to roll-back to a previous version of your anotation state</p> <ul> <li>check out the old version and create a CSV download, then re-upload the older version using import;</li> <li>or contact us for support.</li> </ul>"}]}